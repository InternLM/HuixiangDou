大佬们，请问如何安装mmcv?
mmdeploy 怎么安装。nihui好看么
倒是 ReadMe 里面有个笔误，不清楚那群人是否会注意到
我想cmake-gui看所有cmake option但是我是远程的docker没有DISPLAY。大佬们有没有cmake-gui平替？。只要能list 出来所有option就行
好吧 知识库也没有这种东西吧。。。
使用mim源码编译MMC 时如何控制编译时cpu核心数
比如搞清楚同是 spir-v，为啥 opencl 和 vulkan 不能互通
mmdet 3.0 需要哪个版本的 mmcv
比如说.to(cuda)之后，CPU memory里的会被释放嘛
[旺柴]这模型开源吗？ 。能瞟到不
opencompass是什么？
这个需求对模型的要求是不是有点高。多模态输入 多模态输出
这个在安卓 跑起来快吗？
（有没有群友做大模型多模态啊，现在一般的做法是怎样的）
这得等多少年才能捡到便宜来玩ncnn呀
还有imread这种高级api吗
啊为何不docker呢
repvgg可以自己控制尺寸的。你要压缩到多少参数量
瑞芯微不是自己有rknn嘛
python mp库的进程池怎么inplace修改列表呀
大佬们，npu跑模型，参数是放到内存里吗
意思是前面用NPU跑，出来之后接个别的框架跑MHA？
没懂的一点 都是matmul 为什么620不能支持MHA
我感觉attention的部分CPU介入一下，让后最后projector的部分再打回NPU不行么
有INT16模式么）
这个reshape，NPU也不行么hmmm
他这个场景下应该不会出现零尺寸的tensor吧
请问如何安装 mmdeploy ?
然后里面的setup.py没看懂怎么run的
有啥 zero shot 的检测网络吗？用于工业产线的长尾应用，例如这个月检测包裹，下个月检测自行车。[让我看看]
有没有先用算法过一遍再手调的
讲道理没有人优化一下clip吗，这clip看不到细节有点难受呀
神经元数量够的话  下一步似不似是不是在网络里面添加主动获取奖励的机制  进行迭代。神经元数量够的话  下一步是不是在网络里面添加主动获取奖励的机制  进行迭代
mmcv-lite 一开始是给俺们组定制的，为啥 release 出去了。
mmcv有预编译whl，拉下来装了。就缺mmcv.runner。怕不是新版本又改名字了吧
openxlab 是什么情况？。我在上面下载数据集，看起来也是上海实验室的
请问用了 torch 的话算不算落地用了 cuda
佬们，有啥目标跟踪网络使用的 gridsample op 吗？
佬们。ncnn的运行时开销大不，计算shape alloc这类动作
就把langchain那玩儿直接模型化不行吗
所以mnn最后是因为开发人员都去做llm而放弃了维护了吗
ray也可以做推理？
虫叔解决了服务器放modelzoo需求了么[奸笑]
有大佬教一教用rtmo代码跑pose任务的吗
用的wechaty 付费puppet么
群友们我有一个问题，我用opencv对一个mask做高斯模糊，很慢，有什么更快的实现吗，python上的
有啥推荐的吗？轻一点的 Transformer Detection
大模型啥时候才能反应快一些 感觉可以替换 be my eye
深度模型能整点啥好玩demo吗
opencompass如何调用自己的模型代码？
opencompass如何添加自定义模型？
现在端测llm推理用的都还是torch吗
sys msg。mmdet应该也有排名把
话说白座，现在国内有效果还行的能看图的llm吗？开源或者商用但能调api的也行
啊，啥模型的呀，我也想试试多模态的ocr
类似之前的多塔吗 加一个ocr塔
用mmengine的东西里面怎么跑的多进程？怎么用的registry
白座那个项目后面支持llama2了嘛
他只能禁止反编译，那我从cuda源码编译到别的平台他管得着么
大佬们，yolov5要怎么训才能训出8bit量化后掉3/4个点这种水平
LLM 的适配情况吗。sys msg。LLM 的适配情况呢
llama.cpp就能解决的事儿，为啥还需要专门的大模型推理框架呢
话说现在有好用的全景分割模型吗？segment anything分割出来的感觉太碎了
不过dram内存和ssd的原理好像有点差异 不知道可么
话说对于超分这种模型，做量化效果好吗？
微信早就说有linux版了。不知道在哪
我有个问题啊，这些llm的test都是publicly available的。那后来的LLM是不是直接就学到了
笑笑知道怎么用 LLM 做 function call 吗
你这个计算是怎么算的呀，看不懂。你要不找个啥trace一下给哥们弄成reshape啥的拼起来让哥们看懂一下下
我怎么感觉这给图片有问题
）想请教NV GPU上的一个问题，我在kernel里面的很多参数，比如说thread block数量之类的，那driver是直接像变量一样套用，还是需要重新编译
就，你从API的层面上来说
应该是Reshape Transpose可以不用？
万能的群，有啥模型可以做类似的图像增强？
话说torch这么庞大的东西，他得有多少个专职的程序员呀
所以是不是用int4就更扯了
数据量跟参数量匹配，大的没法直接炼出来，所以小的也不行？
[旺柴]。token好像也是训练出来的吧
这么高大上的facexformer里面为啥要塞一个mtcnn
那种方式的已经学会了。现在是模型就是有好几G这种咋个搞[捂脸]
的量化没做group？
ncnn 量化完了跑的时候是 一层一层解开跑 嘛
传统的cnn算法？难道装个MNN做端上训练？
主要是训练需要浮点运算吧 还要算损失函数 反向传播 端侧有点奢侈吧
能外部输入权重嘛（
水佬 有没有开源的 vlm画图模型啊
俩字只算一个token你咋不提了呢
水佬教我玩量化吧~~
现在代码能力强的模型是哪个呀。能解决动态规划问题的
yolov8pose如何跑coco指标呢
yolov8pose没有coco指标怎么和mmpose模型对比呢
请问我在部署onnx推理时，为什么在同一个py文件中用build_task_processor分别初始化mmdet和mmpose的processor后会导致mmdet_processor.create_input时报错呢？
能问一下，mmpose  rtmpose dwpose他们之间什么关系
刚才还想问mmpose是不是类似于pytorch的框架
大佬们，我看tokenpose-s的参数量挺少的啊，Gflops也还行，为啥部署不用呢？是因为推理速度慢还是因为部署不友好（算子支持差）啊？
一个小问题，SDK的姿态估计示例可以传入物体的bbox，如果在多人场景下，只传特定的某个人的框，是不是忽略了其他人，类似于做单人姿态估计，速度快一点
大佬们可以帮忙看看这个数据配置文件吗
这个有训自己数据集的教程吗？
各位大佬请教，要是使用mmpose训练自己的数据集需要标注的时候，有什么推荐的标注工具吗？
请问是否有将mmpose部署到jetsin xavier nx的教程吗？
请问是否有将mmpose部署到jetson xavier nx的教程吗？
你这样问问题效率很低的，你说了半天到现在连你跑的哪个数据集都不说，让我们掐指一算靠猜么
请问为啥yolov8xpose-p6模型的时候，它还需要下载yolov8n.pt的权重呢。。感觉跑关键点检测的模型，也用不到yolov8n.pt这个权重文件
不知道我要是跑yolov8x-pose-p6模型，是不是要下载yolov8x.pt的权重文件
yolov8pose的map是怎么算的呢，感觉和coco指标的ap差别很大。
镜佬的one stage方案打算啥时候来个preview版本啊
各位大佬们yolopose出现这个问题是啥原因呀
佬们，mediapipe pose是不是速度最快的人体姿态识别的方案了
dw对比rtm有啥优势？除了精度高一丢丢？
dw用的就是rtm的模型，结构啥都一样，速度怎么能快呢
Flow模型是不是都比较少人研究啊[捂脸]
想请问下，bottomup模式下，跑的dekr那个方法，标注的目标框是还在训练中起作用吗？我手动设bbox = []是没法训练的，在想着标注的时候可以不检测框，只标关键点不？
mmpose调试coco的代码时加载数据很慢 这种情况各位有遇到过吗
想问一下调试速度是和底层gcc这些有关吗[捂脸]
是的，你用的是预训练权重吧
打扰大佬，还有一个mmpose的问题，我使用单人的视频，输出的json文件，好多frame里面有两组 key_points, box_score 都是 1，但是第一组的keypoint_score 比第二组高。为啥会输出两组呢？
有大佬知道 怎么提取出 骨干网络 生成的特征图么？
有什么可视化工具么？
只用调用参数use_udp就行了嘛？
请问mmpose算法库 代码能改吗
看文档注册机制不太明白
兄弟们，比如说我在一个数据集上训练了一段时间得到了一个新的权重文件，然后我对这个数据集再加了点图片，想用那个权重文件继续训练新的数据集，使用resume还是修改配置文件里面的checkponit呀
就这种结果图能用mmpose跑出来吗。大佬们
你要不再对着代码理解一下吧。或者看看开放麦的录像
simcc被 RTMHead优化了 所以实现了 高的性能， 那么 rtmhead 在其他模型上会有更好的表现么？
嗯哼？ 所以您的意思是 SimCC 最后 实际上还是 一对反卷积最后才加的分类头 出的效果么？
emmm  就是 设计一个好的Pipeline？
眸佬，mmdetection能支持多数据集一起训练吗？
有用dinov2的原始预训练模型搞检测的吗？
大佬，这茴香豆怎么训练的？后面打算开源吗？
请问为什么那个demo会报错呢？
MMDeploy模型部署-Pytorch转ONNX. 目标检测模型出现'NoneType' object has no attribute 'copy'。问题，求解谢谢
请问豆哥，mmpose 有哪些keypoint loss，每种loss适用什么场景？
mmaction那种模型可以实现多人动作识别？
大佬们，我想用rtmpose 只检测画面最靠前的人，有啥好办法吗？
Mmaction2torchserve有问题吧？
但是看飞浆那边的Paddle 的生态项目里 有这种 mmpose有类似的实现嘛？
只追求精度，rtm是不是并非最优选择
请问一下，在mmpose里边怎么对数据集做采样处理呀？
行为检测模型如何转tensorrt？
有个问题哈，为啥onnx模型在jetson设备上部署，不是直接转成trt模型就行，而是要搞一大堆的mmdeploy的部署[疑问][疑问]
能不能把mmdeploy的算子直接搬到trt?
可以指定mmdeploy的算子？
yolopose关键点检测好像只能识别出 框和关键点 ，如何实现识别前后转动的角度呢
还是那个问题哈，onnx模型在jetson设备上部署，有好的方法直接转成trt模型，避免搞一大堆mmdeploy的部署吗[疑问][疑问]
如果没报错，该生成so的话，那就会在对应文件夹里面有吧
大佬们，怎么办，mmdet的onnx模型里的topk，这是tensorrt算子不支持吗
「JPMark: 「焕军：是的， trt 的 topk 必须要是个固定值。 mmdeploy 里有对应 rewrite 」。- - - - - - - - - - - - - - -。试了下Nvidia自己的polygraphy和convertmodel的转换，不work，老哥你说的rewrite在哪，能给个地址不」。—————————。https://github.com/open-mmlab/mmdeploy/blob/main/mmdeploy/pytorch/functions/topk.py。
各位大佬，我想咨询一个问题，关于topdown与bottom up的。如果我纯输入一张已经识别到的目标图像（通过目标检测器识别裁剪操作，得到的图像中只一个目标），后面在关键点检测中，topdown模型能不能像bottomup模型那样直接使用，不用设置mmdet环节。
小白问个问题，MMdetection怎么能够使用继承torch.autograd.Function自定义的损失函数，总是报错TypeError: cannot pickle 'LossFun' object in <mmengine.hooks.ema_hook.EMAHook object at 0x7fc845e0d6a0>,求各位大佬能帮助一下
各位大佬， 咨询一个问题，模型训练时 batchsize 与 num-_work 两个参数，怎么根据gpu的型号调整哇
各位大佬，我想咨询一个模型训练的问题。我有一组Ms coco格式的关键点数据集，里面标注了66个关键点，但我只想用其中的30个关键点来训练模型，并且不考虑点之间的连接关系。这种情况能实现吗，如何修改配置文件
或者说，没有这个bbox_files文件夹，也能对模型进行训练吧？
是不是需要cmake opencv才能用cuda？
大佬，可以分享一下 OneHand10K数据集吗[加油]
大佬们，我需要高精度的关键点检测算法，时间因素没有那么重要，朋友们有推荐的吗
大佬们  用simcc训练hrnet。占用gpu。为什么那么大
大佬，有个关于自定义数据集的问题请教一下。就是我有一组coco数据集，里面对每个对象标注66个关键点。我计划只用其中30个关键点来训练模型，请问有什么方法可以对这个做处理吗[皱眉]
大佬一个hrnet训练出来，一个epoch占七百多内存，正常吗。[囧]
对一下cuda 版本，检查cuda补丁包装了没
想请问一下rtmpose部署成功后，官方的API推理提供了单张图片示例，但如果想对视频进行推理，用cap循环读取帧图片再cv2读入img做检测速度很慢，有什么更好的处理方法嘛
要不你试试blazepose？
谁有关于遮挡优化的可视化对比图吗[合十][合十][合十]
突然想到，你直接crop不也应该一样能构造不少样本么？
问一下我这输出要咋改。难道再加个卷积。[疑问]。hrnet输出固定的不是
各位大佬，我测试自己训练的shufflenetv2 模型，报如下错误。大佬们知道这是怎么回事哇
大佬们我在装MMPose的时候这个测试我做不出来，是咋回事
大佬们 mmdeploy的虚拟环境能和mmpose的环境用同一个么？
我想问一下 rtmpose 我先训练目标检查 可以不加关键点嘛 就是只有边框的 coco格式 没有关键点
要不大佬。你实验一下。拿coco数据集跑一下。也很快。我看看是不是我自己的问题
我试一下哈。那不是变成低分辨率了
镜佬 RTMPose这个图里面的具体数据能 从哪里获取么？
请问我想在mmpose的训练中，我val的时候，评估两个模型，有什么修改方法吗
这里后面的log里的ap和这里显示的ap对不上啊
请问下mmdetection框架dino算法，backbone只有swin-l，有没有模型更小swin-t等配置文件？
大佬们，这个错误是什么原因啊，mmdeploy-runtime-gpu
有大佬知道怎么找到smpl-x模型中kinematic chains 的实现部分在哪儿吗
大佬们 有人知道yolo的txt格式数据的深度挖掘怎么做吗
请问一下  基于mmpose做的代码  准备提交code给会议，版权或者开源协议这块要怎么弄
大佬们，我跑yolov8 关键点检测报这种错。有人知道怎么解决吗
chunk预测模式，有推荐事例代码吗，想参考下
我的需求可能是预测的类别比较多，我在想分一级类别两级类别预测，减少prompt输入；还是现在输出，prompt输入比较长
话说bert这种语言模型是否也可以在推理时候简单外推下，让他在大概512范围内性能不会下降特别明显？。有没有做nlp的佬。不要求外推很长，大概能够满足需求就行。256确实有点短了
可能得找一下有没有加rope的bert开源，但是何不换个语言模型呢，不一定要用bert吧
大佬们不使用预训练权重在哪调嘞
大佬们问一下。我就加了这个。训练没问题。但是预测时候。TypeError: __init__() got an unexpected keyword argument 'radius'。why？
你用哪个脚本推理的？
想把pose 在multi task的时候集成到vlm 里面，大佬们有啥建议吗？
大佬们，我用自己的数据集（三千多张图片17关键点）用hourglass和hrnet训练（batch size8）不到二十轮精度就0.9了，这是为什么啊[苦涩]
不同类别作关键点检测，如果想先分类再关键点检测，是分开训练好还是联合训练好，不是太懂。如果分开训练(分类模型的数据集和关键点检测的数据集是否需要一致呢，因为感觉关键点检测效果先依赖于分类效果，如果关键点检测的测试集图片刚好出现在分类人物的训练集中，感觉不太行哇)
higherhrnet这种down_top的可以吧？。。openpose这种？
请问大佬们，关键点检测模型比如rtmpose可以判断此关键点是否处于遮挡状态吗
是不是可以结合加机器学习的方法。比如svm异常检测
大佬们，我要往模型后面继续加模块咋办，比如卷积啥的，但是显示是list不让我操作，怎么变成tensor
这个onnx转成rknn后模型的输出是什么啊？
想问下大佬们，rtmpose预测的关检点得分为什么会是大于1的呢，这个得分的范围是怎样的呢
要是要查看模型的参数量是需要去自定义HOOK吗
但这样在创建模型时好像也不会用我修改的模型代码。还是我代码改错了
大佬们？rtmpose的onnx为啥转不成功啊？hardsigmoid怎么避免。官方什么版本转的
有大佬推荐一个比较好跑的backbone模型吗
mmpose关于服饰关键点检测的算法模型在哪里呀
。你这github链接 还是空的
想问问，rtmo和yoloxpose是不是共用一个backbone和neck，只是head不同
使用mmdepoly转onnx报错，是不是暂时还不支持呢？。rtmo
奇怪，我这边用 opset version 11 是可以的。你pytorch，onnx版本都是多少呀
请问mmpose，代码运行过程中数据是怎么一步一步传递的，想尝试改一下数据预处理的代码，但看的时候代码全是一块一块的，捋不明白，有啥方法吗/
请问把脸部、身体关键点模型，移植到手机端，模型轻量化需要做哪些工作，怎么调整呢？
mm新人看了一天的rtmo代码，相比yolox-pose，是不是在neck多少transformer的结构和在head上多了dcc模块，以及loss_mle。mm菜鸡新人看了一天的rtmo代码，想问问相比yolox-pose，是不是在neck多少transformer的结构和在head上多了dcc模块，以及loss_mle
大佬们，想问一下，bottom up关键点检测 以你们复现的dekr为例，这个算法只能接受一类框吗？能不能接受多类框。他是可以接受多类框，然后根据不同框的名字，组织名字相同的关键点生成热图拿去训练是吗？
请问大佬如果我想自己标记一个 小规模的 video 2D关节点检测数据集 ，有没有什么比较好用自动化较高的工具呀？
这是优化的结果，为啥你还想多用点内存
我就调训练配置的线程数，那个pencv thread怎么调？
大佬，不行啊，这该怎么加速训练速度啊？188G的运行内存，怎么也用不到30G，加线程也没用
workers设太高是不是也没啥用。单卡训练的话
有大佬用过onnx的静态int8量化嘛
请问在哪里可以设置一下输出val的loss值呢？
loss非常低有啥问题吗
有大佬试过rtmo 转ncnn 有坑嘛？
sys msg。佬 RTMO 在predict的时候 为啥是直接调用的 YOLOPose 做的处理？
这一行之后 不都是在 检测和解码了么
正常不应该是用你设计的 forward_train来 编码特征图么
有大佬了解吗，运用同一个rknn文件分别用c++和python推理（前处理都相同），推理出的结果有差异吗
大佬们为啥batchsize设置一样。[图片]。一个这么多。一个一百多。一个yolov3。一个efnv2。sys msg。一个主干网络 mbv2
[图片]。啥也没动，就是mmdection配置好的。怎么差距这么大。理论上mobienetv2还轻量化模型。batch size 都是32
大佬，训练rtmpose时batch大于100就会出现这个错误怎么回事啊？明明显卡内存还有足够的量使用。[图片]
我想想。训练数据能撑满bs100么
用小的bs训练完成过么？这个报错也可能是有数据集加载失败
debugpy怎么调ddp啊，佬会吗
各位大佬，topdown方法会在bbox外预测特征点的默认机制可以取消吗
大佬，mmpose有动作迁移的代码吗？
大佬detinferencer 用grounding dino 出来的结果 label 是数字，这个数字 是和text 用.分割一一对应的嘛？
请问，我用yolov5做自顶向下的检测中的nms换成diou_nms，为啥红色框中人物叠在一块的时候，还是一个框。[图片]
您好，请问这个我如果想要用比较新版本的模型去test 推理。 这个我该要怎么么写呢？
那我自定义数据集要哪里修改类和个数嘞
诶如果你加metainfo训练能跑，为啥推理会跑不了
mmseg支持新的config，需要怎样修改
[图片]。训练rtmo的时候报错。有哪位大佬知道为啥嘛？
有没有大佬遇到过用rtmw-l转为tensorrt出来后，置信度大于1了，这是咋回事呢？mmpose和mmdeploy是最新的。[图片]
有没有兄弟说一下这个model_path在哪里[抱拳]
1.x的 generate box 的py 文件去哪里了
感觉你的验证集好像有点弱
这精度看着还行，rtmo用的哪个，RTMO-s？
大佬,rtmo是不是还没给出c++的前后处理代码?
请问一下大佬，mmpose里有测试onnx推理速度的代码吗
请问，现在mmpose的inference可以使用mmtrack来进行检测框的追踪任务吗？
请问有修改过rtmpose网络架构的大佬吗～
不好意思我是小白[苦涩]，请问从源码安装的话，直接修改models/backbone/cspnext.py后，就能用config直接训练修改后的架构吗？还需要其他什么步骤吗？比如什么注册器之类的
或者可以提供代码学习吗（貌似脸皮有点厚了...[旺柴]
镜佬，啥时候出个好量化的架构啊[捂脸]
昂！确实有scope=mmdet，那请问要怎么改过来才直接调用models/backbone/cspnext.py里的呢？
我把rtmo的backbone换成mobilenetv2，试着只在coco上训，但总是训崩，这个有啥训练技巧么。是不是得先在yolopose上预训练
我取了8、16、32三个level的feature map，然后neck改成了PAFPN,融合后不输出stride=8的feature，这个应该没问题吧
有大佬教一教用rtmo代码跑pose任务的吗。有没有相关的学习视频
请教大佬们：将cspnext的标准卷积换成深度可分离卷积之后，batch=1也报错RuntimeError:  CUDA out of memory。。这是为什么呢？按理说换成深度可分离卷积模型计算复杂度变小了呀
视频超分网络 mmdeploy支持了吗
你看一下mmskeleton上次更新是啥时候
不过我是用的mmaction2，发现不太对，所以用论文进的源码，发现还是这样，所以有点疑问
请教一个问题，我想用mmpose重新训练一个checkpoint，resume之后，他的学习率还是初始的学习率，但是这个checkpoint的学习率应该是一个更小的值（用了余弦退火，这一轮的学习率和一开始的学习率不一样）。这种情况应该怎么办
请问下 mmpose有没有算法硬件适配相关的说明啊  比如在哪个移动端芯片上进行实测一类
有大佬了解理论方面的知识吗，原本的yolopose是检测人体17个关键点，现在本人的数据集不是人体中的17个关键点，是另外10个点。这应该是对原yolopose中的哪一部分进行改进了吗
镜佬，视频版本关键点模型，有后续跟踪+滤波示例吗？
这个输出不是有个384 512吗，改了inputsize这个要改吗
有大佬知道，rtmpose这个脸部模型，需要用哪个detection模型吗？
大佬们，C++实现fc的矩阵乘法有什么好方法吗？rtmpose的最后三个fc层我必须要把输入拉成一维才能在我设备上量化[捂脸]
感谢各位大佬 上个问题解决了[Salute] 有一个新问题 我用“wholebody”测一个没有人出现的片段[捂脸]，保存的json当中几乎所有的bbox_score得分都是1 ，这个1是什么意思呀？
你可以找下有没有对量化友好的位置编码方法
lmdeploy中cache-max-entry-count代表什么，和显存占用的关系是什么？
请问如果在其他项目训练好了目标检测器，想要和mmpose里的top-down方法一起用来推理，能够实现吗？有大佬试过吗？
代码里是对的。网页上面链接错了吧
首先你得看能不能检测到，其次我感觉再看能不能检测到关键点
那这种目标检测分类应该能弄出来吧
我在尝试学习使用mmpose自定义模型，自定义模型的主干网络的话，主干网络的输出需要是什么样的才能和颈接到一块啊
请问哪位大佬有deep image matting数据集，求分享[抱拳]
难道用gt训练出来的，实际检测的时候结果更差？
请问有朋友统计过Hourglass-52的参数量和FLOPs吗？能正常训练，但是统计参数量和FLOPs的命令跑不通
请问运行tools/test.py输出的可视化图像，一张图像上只能呈现单目标的结果吗？
我想的是不知道群友有没有人造过轮子把mmpose跑的结果改成标注 再去精修的。emoji
以及 它的效果能超过openface这种专业模型吗？
有人在用yolov8坐关键点检测吗
mmpose 除了能拿到关键点坐标信息还能拿到什么数据信息嘛
按照说明注释掉了这些东西是不是训练和测试的时候都不会进行rescore了？
请问一下各位大佬，有没有关键点检测数据增强的案例啊
请问有人用过SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation这篇网络的代码吗，想交流一下，谢谢。主要复现不出trainforscratch的性能
佬，你们用什么部署的
你是没有加自己数据吗。这种情况下的数据
拿学生的视频标注深度？然后再训练是嘛
验证集不需要同步增强吗，我不太懂，那这样的话增强训练集的同时验证集始终很小。我再查查。谢谢大佬
有大佬用过mmaction2嘛
[可怜]问一下，做金丝猴的姿态识别要识别的关键点有哪些？包括尾巴吗？还有流程是先标注关键点，然后弄出对应的.json文件做训练集和测试集，后面是进行目标检测和关键点检测就结束了吗？
请教大佬一个问题，mmaction2的demo_skeleton生成的视频为啥打不开
大佬们，mmcv和mmpose可以安装在深度学习的环境中吗？还是要单独建一个虚拟环境？
这个安装mmpose，显示‘git’不是内部或外部命令，也不是可运行的程序或批处理文件。这怎么弄？。[图片]
对物体进行关键点检测用yolo好了还是rtm pose好呢
大佬，git base这个命令窗口怎么打开我创建的虚拟环境呢？
大佬们，在虚拟环境中安装mmpose，已经下载了git，也配置好了环境，在anaconda的命令窗口使用git命令还是不行，这怎么弄？
你用的windows还是linux
那官网安装教程在虚拟环境中搞这个命令是干嘛的？新手不太懂[破涕为笑]
豆哥在我修改网络模块时候yolo.py中parse函数应该如何书写 网络卷积.yaml文件应如何修改
验证MMpose是否安装正确，怎么弄？
训练的关键点检测模型oks怎么看勒
mmpose里有哪些算法模型是transformer架构的？
mmpose有测试推理速度的工具吗
训练结束了。应该有保存在哪吧
为什么没有那个scalars.json文件呢。可视化那个
请问大佬们有在工程部署中，有多个pytorch模型使用多线程来同时推理？
请问大佬一个问题，Action Recognition可以是行为识别嘛
有人在mmdet3d群吗 可以拉我一下
rtmpose有没有使用纯onnxruntime部署的教程？
我就没有那个json文件呀，那是训练生成的吧
如果是aarch64，按照android的教程编译就可以了吧
请问在模型部署的时候，图像的预处理和结果的后处理都会被包含到模型里面么？
大佬们有同时基于不同的backends（如nvidia和amd）训练过大模型么
就想问问你们，把一个大模型基于模型并行或者其他的策略，同时基于nvidia的gpu和其他的device（如华为、AMD）之类的，多机多卡，有这么训练过的么
关于网络冻结 冻结第2层 然后第1层还能传回梯度吗
不知道诶，你可以问问豆哥
感觉mmdet里的voc 评估器是不是有点问题。总是卡住不动了
眸佬刚刚说今天放出来的paper是什么
mmdet是不是还没支持tinyperson和visdrone
也是个好问题，把task定义好
对于segformer模型有一个问题，PatchEmbed最后是一个layernorm，然后接下来计算的block中的一个也是layernorm，相当于是两个layernorm挨着这是为什么，没想明白为什么这么设计[捂脸]，官方的和mmseg都是这样的
我主要是看到mmEngine集成了它。就想问问效果咋样
请问下，mmseg 训练的时候图片大小如果是 64*64，那它预测的时候是怎么分割更大的图片的呀。。我看 unet 在 drive 数据集上是 64*64，但是我在推理的时候，使用更大的图片也能完成分割，想请教下是怎么做到的
是因为我用的是 unet 吗，所以可以 size 无关
问一下 你怎么看openmmlab
但是这个不是数据增强后数据的可视化嘛。我是想知道具体经过了哪些数据增强
群里有人做过交互式目标检测吗？或者有一定了解吗？
类似sam那种交互吗
想问一下，怎么设置train_cfg可以让前n个epoch训练dataset1，后m个epoch训练dataset2呢
帮问，mmdeploy c++怎么设置占用资源，比如线程数
mmagic有finetune的指导文档吗
有没有可能你会转成coco也就会处理数据了
请问一下如果想修改mmengine里面的sd。我应该改哪个文件
我看mmengine的文本编码器和stablediffusion是封装到一起的，改怎么去修改文本的编码器
Openmmlab以后会有强化学习的库吗
请问各位。mmdet怎么使用fp16训练，用了ampoptinwarpper，参考了mmengine。显存不降反增
插队问一句 大佬们知道有哪些好的polyline检测器么
可以对照demo代码写吧
佬们，请问如何在mmdet里面支持新的标注数据集
请问mmseg里面有没有膨胀推理的功能
大佬们~mmpretrain里，swin-transformer-v2，官方权重转换mmpretrain的代码有嘛[旺柴]
要是写pr稿，标题我昨晚都想好了：  你真的超过llama2 了吗？
请教一个问题，模型在forward的时候可以过去runner的当前迭代次数吗？还是必须重写一个forward把runner作为参数传过去呢
不懂就问：POE里就有chatGPT3.5等一众语言模型， 可以替代ChatGPT网页了？
mmdet什么时候支持Mapillary Vistas数据集啊
第一次写分布式训练代码 想问问这种每个epoch开始loss会有断崖式的下降正常不 只保存了rank 0的log
有没有比swin transformer更好训练更高效的backbone推荐呀
。朋友们请教个问题, 这个python简单调了十几个库, 怎么分析都是那些tensor 或者变量占了多少显存
detclip 请问大家有见过开源版本么？
请问这儿指定rk3588后后端自动将onnx转rknn去run吗？可不可以让我自己提供rknn模型呢[旺柴]
求助各位带佬，eva-vit模型，用torch.onnx.export导出onnx，结果出现这么多文件，而且onnx只有360k。咋肥事捏~。求助各位带佬，eva-vit模型，用torch.onnx.export导出onnx，结果出现这么多文件，而且onnx文件大小只有360k。咋肥事捏~
可能是输入图像的尺寸有点小吧
能问下大家有办法修改list[dict]里面的元素嘛
能问下大家有办法修改配置文件里的list[dict]里面的元素嘛。那只能多加个参数写代码里了[苦涩]
上几千个核心的超算应该能快点吧
看MMEngine源码？
有大佬了解吗，真的可以比得上gpt4吗
有大佬玩过 retnet 吗？真的可以取代 transformer 吗
不懂就问，那未来的发展方向是洗数据吗？[让我看看]
如果效果大差不差，研究这么多网络干嘛呢[捂脸]
我导师在做多任务的时候，其中一个任务，就直接用一个很简单的 MLP 网络去做分类，所以我有些疑惑，感觉这个网络比较简单，我当时就在想为什么不用VIT或者更复杂高级的网络呢
是用的统一内存api吗。可以超量申请
最近整了一些大模型的教程有佬想提前体验一下的吗[让我看看]有算力支持
请问下，mmdet的test.py，如何让其导出推理结果而非metrics？是不是必须自己写一个runner和TestLoop类的run方法才可以？
看下本地 cuda 版本和 torch-cuda 小版本是不是一样。
请教一下佬。目前在做一个医学影像处理的小工作。在做语义分析的时候想自己做一套数据集。但是niftii这种数据是个三维的感觉 似乎不同于以往的jpg这种一张一张的 该怎么标注得到mask呢[捂脸]
能问下有办法可视化DETR系列的encoder和decoder的特征图嘛
大佬们，mask2former里面，这些不需要配置嘛[嘿哈]。怎么默认知道是Mask2FormerTransformerDecoder
请教大佬们，对于开集检测，有那种“输入对于一种实体的描述，然后输出一个或者多个对应这种实体的检测框”的视觉任务吗
是不是类似detGPT那种
我直接用的mmcv里实现的，应该也没有什么问题？
能问下各位佬vscode可以直接调试多卡的代码吗
data_preprocessor中的crop_size？
data_preprocessor哪来的crop_size
想问下佬们，mmdet里面有好用的测fps的工具嘛？。sys msg
佬们~mmseg 1.x ，如果要支持新的config，就是支持config里跳转的话。只需要把包引用进来，然后把字符串的符号去掉嘛？还有什么地方需要动呢?。[图片]
大佬们，在尝试新config的时候，报错这个。。绿色框，是打印输出的dataset_cfg的内容。[皱眉]可能是什么问题呢
[图片]。看起来是数据集 config的 问题嘛？
能再说说看 meta device吗
大家有遇到过这种多卡训练的时候一个epoch只log一次的嘛，况且训练得特别快，感觉应该没有训练到1。sys msg。大家有遇到过这种多卡训练的时候一个epoch只log一次的嘛，况且训练得特别快，感觉应该没有训练到12个epoch
估计是在train循环里面的bug？
请问图文对的dataset是不是无法下载本地。（无法hf cli下载
nuscenes数据集在火山云上，本地如何读取他的pkl文件，有大佬能指点一二吗？
大佬们好，请问一下，一般来说，数据加载是目标检测模型训练时间的瓶颈吗
想问一下 我做的目标检测数据集，用的mmdet的数据集标签格式，怎么使用Coco的标准来计算mAP。在mmdet里没有找到相应的类[破涕为笑]，看了一下自己写评估代码的话好麻烦呀[捂脸]，有没有什么办法
有好兄弟跑过llava的mmbench的evaluation么，我就用官方的权重复现，发现分数比论文里还高了，大家有试过的么
是不是得先在yolopose上预训练
agentlego的工具大部分也要用huggingface的模型，（因为网络问题下载不了）请问离线模型应该存放在哪个位置[玫瑰][发抖]
有朋友调用过 whisper v3嘛  无论是 api 还是本地方式 （想找一个封装好的接口作视频转录
有个事想请教大佬们。俺在变化检测这种小领域 应用了一种全新训练模式和寄生模块，用93.74的精度跟base的90.26拉开了差距，并打败了小领域内SOTA同一个backbone下91.86的精度。这种开山之作的工作有能力发顶会吗
请教下各位大佬。我最近在做一个工服识别项目，用分类算法，目标工服是上下装 纯蓝色，是一个二分类问题 是/不是目标工服。在制作数据集的时候有个疑问:只准备两类数据集——蓝色和其他；还是准备红橙黄绿蓝靛紫等十多种衣服类别数据来训练模型
话说白座 最后rage这个架构师折腾了多久弄出来的
为啥这cogvlm的specialist模型才比generalist模型高了一点点。[图片]
mmseg里，如果测试数据集的尺度差异很大，512*512也有  1024*1024也有 1w*1w也有。那么这个值可以让 图片是多大就设置多大嘛。[图片]。把Resize去掉嘛？
豆哥，如果一张图是1024*1024输入，Resize=(512,512)，test_cfg=dict(mode='slide',crop_size=(512,512)，stride=（384，384））是不是没用呢
想请教一个问题，我在用mmaction2训练的时候，GPU温度快超过正常范围了，并且功耗也超过了正常的范围，请问这样该怎么解决？显卡我用的是两块2080TI
「MMSeg-Dominic23331: 想请教一个问题，我在用mmaction2训练的时候，GPU温度快超过正常范围了，并且功耗也超过了正常的范围，请问这样该怎么解决？显卡我用的是两块2080TI」。—————————。57 度还行啊？ 没到 80 都不用担心。
「MMSeg-Dominic23331: 想请教一个问题，我在用mmaction2训练的时候，GPU温度快超过正常范围了，并且功耗也超过了正常的范围，请问这样该怎么解决？显卡我用的是两块2080TI」。—————————。看截图， fan 就 57% ，你没发温度图。  如果温度超过 80 才可能是问题。
「MMSeg-Dominic23331: 「焕军：「MMSeg-Dominic23331: 想请教一个问题，我在用mmaction2训练的时候，GPU温度快超过正常范围了，并且功耗也超过了正常的范围，请问这样该怎么解决？显卡我用的是两块2080TI」。—————————。看截图， fan 就 57% ，你没发温度图。  如果温度超过 80 才可能是问题。」。- - - - - - - - - - - - - - -。温度已经超80了」。—————————。那检查主板，扩展新散热。 最简单的，找个大风扇。
豆哥是焕军老师的爱好吗，还是InternLM团队的分配项目啊[捂脸]
想问问大家有没有遇到过WSL2不能克隆GitHub仓库的情况 网上的解决方案对我完全没有 而且我能ping上Google但是ping不上GitHub
请问pytest在测试的时候，如果通过了测试样例，但是还希望把print的内容输出到终端，应该怎么写呢？
Openmmlab是不是有个AI 实战营，有提供算力的吗？
有没有大佬熟悉mmdetection里的AutoAugment机制 想知道它具体是如何运作的
如何设定bs 达到效率最高点？ 默认开启了compile的情况
如何更多提高训练速度呢 也想知道 就简单做实验跑跑 相同策略测一下精度而已
想试试mindformers吗[旺柴]
别讲应用，讲点关键点怎么算就行。
我想知道想设计mm这种架构 或者更简单些的架构需要学什么
请问：梯度累计 对loss的影响是什么呢。除了可以平滑。防止局部最优
这样吗，主要要做代码迁移，想看看论文[皱眉]
谢谢佬，mmdeploy是能转换就能部署还是转换大于部署呀[发抖]
想请教下各位大佬，我自定义了一个backbone，也注册了，在train和val时都没有什么问题。在test的时候报错说我的backbone is not in the mmseg::model registry。想问下这个问题如何解决呀[捂脸]
请问下mmcv的new config，自定义字段可以使用class作为value吗
mmdetection中有没有训练好的，KITTI数据集中行人检测的模型
yoloworld精度会比dino差很多吗 好奇
问一下mmdetection的，现在给他打补丁，用哪个branch？
这个应该是看opset的版本吧
可以model输出那里打个断点？
各位佬这里是什么意思？，我没太明白，就算是加了lazy import，解析时不也是需要torch去解析sgd吗？
问个问题，板子只支持8bit整型推理，输入是语音类的连续变量怎么处理啊，有大佬知道怎么处理这类问题吗
现在刚上手xtuner  不知道需要改哪些地方。emoji
请问下为啥mmdet里的有些模型只需要跑1x 2x个epoch 一般不都是2 3百嘛
好的 谢谢 我看有些检测模型需要epoch好几百 有些只需要2x 这有啥说法嘛
mmdet里有什么好办法，让一个模型同时在不同的验证集上进行测试吗[捂脸]
Concatedateset不是把几个数据集拼成一个大的数据集吗？可以分别给出在不同测试集上的测试结果吗？[捂脸]
佬们，这个分析analyze_results.py怎么调用gpu进行推理啊，好慢
这样啊，为啥处理4000多个结果要三个小时这么鬼久的[破涕为笑]
mmdeploy是什么项目，lmdeploy吗
豆哥，mmdeploy是什么
Cosine Annealing 和 Warm Restart 是不是目前比较推荐的最佳实践？（非大模型）
我在想 这种正负样本的思路 是不是可以用在生成文本上 感觉好像有用 又想不到怎么用
我最近也要做一个意图识别 大佬求带
请问 mim install 比较慢是因为啥呀, mim 会使用pip 的国内镜像吗
mim 好像可以加上这个 feature ?
看看单个数据的大小还有numwork的设置
你分别看一下 batch=2 与 4 的
老哥们，mmdeploy可以用tta吗？
是要 小图推理 再合并嘛
有自己搭slurm的教程推荐吗。emoji
slurm不好调试吧
我看好多都是用torchserve部署所以有这个疑问
这个茴香豆怎么触发的
大佬们，怎么在月球上使用mmcv
话说我有一个很奇怪的问题，我有4个线程分别算四个不相关的东西，如果其中一个线程突然之间不用算了，其他三个线程会算得更快吗？
万能的群友，RK3588 这个 YOLO 192FPS 是怎么跑出来的？
佬们 resnet50 224分辨率 计算过程中总访存多少啊
你们可编程性咋样 能写cuda吗
[旺柴]求问，有木有什么比较适合端侧的效果比较好的backbone。现在大家都用啥啊
我看看cublas 怎么修
所以我应该优化做特征的方法？。例如不用句子做特征，而是句子的关键词？
224*224的jpeg才多大
现在模型的信息量值得这么大的模型吗？。是不是我们用的模型如果极限压缩到真实信息量都可以很小。
说起来clip可以用来做图片的语义关系比较么？。直接比两张图
之前是不是有个啥cnn网络用大卷姬媲美vit的
话说在相机里跑clip 这是不是也叫事件相机
我记得木老师说要给ncnn pr个什么东西？
佬们工控主板有什么推荐的吗 rk3588 可以吗
话说mmcv-full现在还有没有呀？。我看现在都是mmcv和mmcv-lite
现在大家cv都用vit了？yolo不香了？
佬们 请教大家个问题 MM框架咋样可以可视化Transformer的attention map呀？
TP除了gather reduce broadcast还有啥
话说我有一个问题，多个数据做推理，是batch推理快，还是用for循环batch1快？。会有明显的速度差吗？
分布式训练用啥框架。ray咋样
走佬的h100用的什么版本的cuda 和cudnn
mmlu这个llm数据集有没有能复现准确率的代码？我那transformers直接在a100上跑也复现不出来GitHub上的50几的准确率。我自己跑就30多
佬们，现在有可以在1080ti上就能跑起来的开源llm项目可以玩一下吗
所以这种ocr很强的多模态模型是咋做的。加一个ocr的头揉进去吗
其实我一直有个疑问 这140g参数 和40g kv 都需要共享吗？。参数应该不需要共享。kv都需要共享吗？
话说白座我有一个问题，如果不给LLM任何的初始prompt，就给一个随机的token，它是不是就会按照这个随机的token开始胡说八道起来？
VLM模型怎么玩鸭 求指导
话说不反向传播用什么更新参数…
豆哥能认图了吗？
才发现这是个llm  怎么触发的呀
rtmpose做关键点训练的时候的bbox的值用真实值来训练的吗？
mmpose现在不能跑训练吗？
有大佬知道生成出来的human mesh 会根据人的身高调整大小吗
训练的卡数和bs啥的跟官方一样么
佬们，请问这里的激活函数有哪些选择，哪里能找到
豆哥还会撤回？
豆哥：今夕是何夕
这个框是需要将目标包含住吗？
请问茴香豆是chatgpt机器人吗[呲牙]
所以是 越大的模型越需要精细化的处理么？
不知道你哪里看到的有反卷积
咨询一下 就是我搞背部骨骼点检测 mmpose yolo 这两个那个更有优势 他们对数据集要求是多少
gpt能有这准确率吗
茴香豆的训练数据有哪些？
为每个骨骼点做标签时，不是在真实坐标的地方设置为1，用高斯分布，然后逐渐向四周递减，直到变成0吗？
mmpose做超声图像关键点检测，要求精确度比较高可以做到吗？
豆哥 mmpose的万物生姿 活动 今年有么
几个检测框还是一个检测框好？
模型第一次运行速度慢，第二次以后速度就比较快？这是为啥
simcc的论文里我记得是把长宽拉长N倍的bin，这是从bin映射回像素导致的吗？
又在其他数据集上测试不。ap10k啥的
mmaction 中 Skeleton-based Spatio-Temporal Action Detection，有什么办法可以转变为tensorrt吗？。
我在load_param之前设置了use_vulkan_compute，正常运行了，不过速度比cpu还慢，这正常的吗？
各位大佬，下午好，我有个obj的三维人体模型，可以通过什么方式来测量人体数据啊.
逼近过程中减少量化误差？
检测出点，还算不出大概的角度吗
我用coco的配置训练出来的模型速度比body8的模型速度快正常吗？为啥会这样？。
🤔config是新版还是旧版
我找不到这些钩子的参数
朋友们，rtmpose如何设置多卡跑？或者设置索引为1的显卡跑
请问各位佬！有没有肢体动作识别的模型？。或者这方面的研究
你看你的work_dir在哪个目录下
问一下我图片推理出的坐标在哪看呢
你数据量和batchsize是多少
难道是因为simcc把任务分成xy分类，就会乱飞？
请问rtmpose保存的最好权重，是保存的coco指标ap最大的权重吗。
是这个headboxfile吗
请问如果用自己的数据集，只标注了五个关键点，sigmas的值应该怎么计算呢。
这个乱码的问题可以通过改DecodeOutputStreamer来修复吗。DecodeOutputStreamer。
问题是关键点数量不一致
佬们有做机器人或者slam vio的群吗
想问一下各位大佬，吧ubody的3d数据做完，训练完wholebody3D如何配置到motionbert，生成带有双手关键点的3D骨骼点呢？还是说只能转成h36m格式的17点呢？大佬们尝试过3Dwholebody么？
有大佬试过one  stage 的mesh 恢复 可以做到多少fps吗？
请问一下有没有做过coco wholebody实验的同学，显存占用如何
mmpose里面支持BlazePose。吗
请问关键点两阶段的算法意思是模型只能检测关键点吗，如目前发布的RTMPose，HRNet。关键点单阶段算法的意思是模型既能检测框又能检测关键点吗，如YOLO8-Pose吗。
请教一下，一阶段模型不会给人的bbox了吗？
请问把脸部、身体关键点模型，移植到手机端，需要做哪些工作，怎么调整呢？
各位大佬，点的类型 upper和lower有啥区别吗？
大佬，mmpose返回值tuple，但是我想在后面继续加操作要怎么搞各位大佬，我有一个关于特征点检测的问题想咨询下。我利用自己的车辆特征点数据训练了HRNET模型，用来提取车辆图像的特征点。我测试了下，发现预测的有些特征点在bbox外面，这种情况正常吗？。[图片]。[图片]。例如像这种
可以根据3d骨架图生成虚拟人嘛。佬们
兄弟们，3d骨架的数据有标注工具嘛？。emoji
兄弟们，3d骨架的数据有标注工具嘛？茴字有几种写法？
群里有大佬复现posegpt嘛？
大佬，训练200epoch的rtmpose，感觉他们变化很小，变化很慢啊，120epoch了loss_kpt:才0.944左右，acc_pose也是一直在0.896左右，这训练正不正常阿？。[图片]
你纠结loss的值干啥
是不是bs太小了
要不要下午给你视频，你试试看看是不是阈值的问题？
我想问一下mmposr最新的版本能用mmtrack进行自顶向下的框追踪吗？。我想问一下mmpose最新的版本能用mmtrack进行自顶向下的框追踪吗？。sys msg
猪猪还要行为识别吗百度云找了一个，这个靠谱吗？。有大佬下载过human3.6m这个数据集吗？
各位大佬，我想了解一下metain元文件中的joint_weights与sigmas参数的大小是怎么定义设置的？是不是joint_weights越大，模型越专注训练这个点，sigmas越大是不是表示在这个点越难学？。[图片]
大佬们，怎么看yolo是否过拟合？
大佬们 hrnet 模型后面加udp 或者 dark后缀，是表示什么意思哇
[图片]。飞哥我想问下在训练阶段。[图片]。这块是进行归一化、padding和bgr2rgb操作，那。[图片]。pipeline这块是在哪完成的呢？。[图片]。飞哥我想问下在训练阶段
佬们，请问hrnet训练自定义的关键点数据集的时候，是不是要修改损失函数呀，因为之前hrnet的损失函数是根据真实标签和预测标签的置信度使用均方差计算得到的，如果用自己的标注的数据集，没有真实标签的置信度呢？咋办啊[流泪]。也不知道理解得对不对[困][流泪]
大佬们请教个修改网络架构的问题[社会社会]。我想修改models/backbone/cspnext.py里的网络架构，没有改类名称，但是改好架构之后好像没有被调用。。。我试着在CSPNeXt类模块的前中后print语句，发现只有前后能打印，类中没有打印。。。但如果类中有语法错误，是无法训练的。请问是我修改的文件位置不对吗？还是改完之后还需要什么操作才能确保被实例化和调用呢？
mmocr可以做到吗
请问一下关于关键点检测的oks指标的关键点归一化因子，如果是自定义数据集的话，这个因子该如何去统计呢？
不知道大佬们，对图像的矩形如何去计算距离和角度，有什么想法没
群里有大佬做3D人脸重建相关的工作么，或者有人复现过3DDFA这个项目么
想问一下，er nerf数字人这种 推理的时候头部姿态没法自动生成吗？
比如取pca参数的topk？
请问RTMO训练的模型，检测框的mAP是不是比yolopose差
我是检测其他目标，这个长宽比会有影响吗
这个sigma，点数不一样需要注意什么吗
会不会是我用的权重太新了？。sys msg。会不会是我用的模型太新了
rtmo要是训练多类别的话该咋训练
rtmo是不是不支持多分类姿态识别啊
多目标应该是直接可以的吧
[图片]。老哥们请教个问题，像这种较小的人体，姿态估计能行吗
如果能检测到姿态估计就有可行性是吗
现在我感觉不是跟踪问题。而是能不能检测到
所以说你数据集咋整啊
首先你数据集哪里来？数据集数量还不够，人体太小，俯视检测效果较差很难达到预期，如何确定是否溺水？我游泳姿势不好看和溺水似的那是否我就是溺水了不会误判吗？
姿态估计模型训练的时候为什么不用目标检测的ground truth
大佬们，人体姿态估计在医学领域的文章有吗？
大佬们，人体姿态估计目前最新的模型是啥呀
大佬们，人体姿态估计方面发小论文容易吗？比如3区以上[敲打]
你好麻烦能问一下pose_tracke这一块的教程在文档那一块儿？[脸红]
你是说半监督嘛
话说姿态能识别出来，那动作的含义如何弄出来呢
什么意思呀。3d不一样也不能都是1.0吧[快哭了]
不过我并没有给他输入2d坐标呀~直接调用了这个接口
大佬们配置文件的8xb32中的b32我知道是batchsize=32，8x是啥意思啊
我数据集240张有没有可能是数据集太少了，我预测效果不太好。[图片]
我们现在就是用一些关键点检测计算相关角度，但是会有一些比较僵硬的动作没法评测出来，不知道大家有没有更好的思路[可怜]
[图片]。[图片]。这里在计算loss时，关键点数量这个维度在变，是哪里出问题了吗？
有没有人画过人体姿态估计的attention map?
树莓派可以使用rtmpose嘛。有老哥试过嘛
在具体任务上，可以认为是 只是对序列的标注不同嘛
比如，行人过马路，我可以标注成过马路的动作，也可以标注成过马路的意图嘛。是不是一回事
请问下大家，目前有没有什么好用的基于rgbd 或者双目 3d 姿态估计方法
我采集的图片是1920*1080的是不是输入进去就变成640*480了
请教一下各位大佬，骨架数据类别不平衡的时候有什么好的处理方法吗
请教一下，MSCOCO，想做smplx格式的伪标注，怎么弄？或者能找到现成的吗
那这些预处理步骤能加速么？有没有什么方法或工具
repvgg这种可以应用到反卷积吗
他不是主要用来做上采样吗
repvgg不也有下采样分支吗
为什么安装的时候一直在降版本
浦源平台有3D Gaussian Splatting项目吗
这种应该有办法改吧。只是默认在这个地方。我写推理的时候。就经常在那卡一会就很烦
豆哥今天心情怎么样
豆哥在干什么啊
有朋友 研究 hanlab的 efficientvit吗？
单纯利用关键点是不是不可以了[破涕为笑]，通过关键点坐标关系进行解析
我们还没发布预训练权重，你是咋用的
或者物体当前的姿态，旋转了多少度
明白了，那还有就是一个视频里面多个人，那识别出来的关键点坐标怎么区别是哪个人的呢
请问mmagic里的lora stable diffusion支持FID score吗？
问一个细节的问题  语义分割哪些label彩色图 是特意保存为8位深度的吗
我看其他数据集的label的彩图 都是8位的。我自己做的png就是24位的 。他们是特意保存为8位的应该是
对了白座 长文本首token加速你们有什么心得么
有没有佬写过关于位置编码理解的文章，或者有没有关于这方面理解的文章推荐
大佬们有啥好用的开集的多标签图像网络吗
兄弟们有跑过多机多卡吗
有没有佬知道训练时加载完权重卡在这里了是怎么回事啊
不是改config吗？。[捂脸]
豆哥这是什么触发机制
大佬，你们领域有没有那种通过改变底层的算法，提出一种方式，然后促进其它领域任务解码的指标提升
大佬打比赛用到了 基于人类反馈的强化学习 和指定微调吗？
这个部署推理。会默认检测cuda吗
How does LVM perform in medical image？。[旺柴]
这个channel_reduction层是原来SAM没有的吧
多模态茴香豆，可以考虑叫MM豆？
请问模型推理的时候，预处理支持一次处理一个batch么？我看现在还是循环每张图片做预处理？。[图片]
请问一个问题 RLHF 在之前就没有出现过吗？
我还想请教一个问题是，这个理论在视觉类大模型、甚至是更多模态大模型上 也是有效的吗
想问问大佬们医学benchmark和diversity sampling for rlhf哪个研究方向比较好
茴香豆是随机找聊天记录来回复吗。。
可以问书生2.0，知道豆哥的茴字有几种写法嘛[捂脸]
豆哥，怎么微调一个写国企八股文的llm
我想知道，豆哥在NLP那边会投顶会吗
茴香豆的logo怎么绘制的呀？
mmpretrain有VMamba的计划吗
豆哥，没有手怎么办
请问一下，类似于身份识别任务这种，网络最后输出的是一个特征向量，最后通过计算余弦相似度来确定结果id的这部分并不属于网络。这种网络能用GradCAM or GradCAM++等来进行可解释性分析吗？。[可怜]
你们是不是看不起豆哥
豆哥 教我怎么制作一份大模型。关于猫娘的大模型
大佬们，人脸检测哪个算法速度快一些
豆哥会领红包吗
豆哥怎么知道的
开源版的豆哥有这个功能吗
请问 有什么llm 看论文的轮子 不会产生幻觉的吗？
为什么你选取的是mmpose来触发算法
大家有没有好的方法能学习到BLIP2 QFormer的代码，他们的源码和hugging face里的代码太难查看了[破涕为笑] 跳转太多 都记不住
豆哥还能把两句话连起来了吗
被豆哥回答过问题，是不是属于参与测试和开发了
为什么豆哥不是996
怎么给豆哥 提问呢
怎么在群里面部署一个豆哥呢，有类似的项目可以玩一玩吗[旺柴]
这个豆哥是怎么触发的
说起来）上次问的，620Q上有木有啥现成的clip based的模型可以demo一下的）。最好是seg或者detection啥的
大佬们mmdete3d可以生成PR曲线吗
gemma有啥推荐论文看吗
我刚接触这里，要做图生文任务。我看到训练多模态模型都是将文本描述和图像一起作为模型输入，然后编码解码后得到文本描述。。那么做图像描述任务该咋做呢？输入是图像，输出是文本。和原始的输入方式（图+文 输出 文）不一致呢
