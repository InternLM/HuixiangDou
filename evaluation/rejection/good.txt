大佬们，请问如何安装mmcv?
mmdeploy 怎么安装 nihui好看么
倒是 ReadMe 里面有个笔误，不清楚那群人是否会注意到
我想cmake-gui看所有cmake option但是我是远程的docker没有DISPLAY。大佬们有没有cmake-gui平替？ 只要能list 出来所有option就行
好吧 知识库也没有这种东西吧。。。
使用mim源码编译MMC 时如何控制编译时cpu核心数
你们的税收，做了这么个东西出来，体验如何🤷‍♂️
b 站还是哪里有个讨论曝光和噪声的，还有栗子，我忘了叫啥了
比如搞清楚同是 spir-v，为啥 opencl 和 vulkan 不能互通
dynamic vision sensor?
佬们谁会用这个软件啊 求教
mmdet 3.0 需要哪个版本的 mmcv
比如说.to(cuda)之后，CPU memory里的会被释放嘛
[旺柴]这模型开源吗？  能瞟到不
opencompass是什么？
这个需求对模型的要求是不是有点高 多模态输入 多模态输出
这个在安卓 跑起来快吗？
这不是ppt还是word文档画的吗
（有没有群友做大模型多模态啊，现在一般的做法是怎样的）
这是不是就是把核显给拿出来了呀[捂脸]
这得等多少年才能捡到便宜来玩ncnn呀
教科书里面也有吧
北京有了解win上逆向的大佬吗 或者win动态调试的大佬
白座不是在搞 nlp算法吗
还有imread这种高级api吗
啊为何不docker呢
repvgg可以自己控制尺寸的 你要压缩到多少参数量
应该不是从我们群学的 吧
那岂不是能用tengine
遇到问题怎么办
瑞芯微不是自己有rknn嘛
python mp库的进程池怎么inplace修改列表呀
之前 zeku 清理资产，你们没去捞点吗
DSP，你们被 cadence 敲竹杠了吗
佬们我抛出一个问题， 我想实现不同光照强度下，图像的亮度不同， 除了固定模拟增益， 数字增益， 积分时间， 白平衡。 这些还需要调试啥吗？ 说简单点就是想固定isp pipeline   图像亮度全靠外界补光。
说起群友有什么看原理图/pcb的diff的东东嘛
大佬们，npu跑模型，参数是放到内存里吗
nncase 文档里的 build 部分怎么直接起飞到步骤 5 了
意思是前面用NPU跑，出来之后接个别的框架跑MHA？
没懂的一点 都是matmul 为什么620不能支持MHA
我感觉attention的部分CPU介入一下，让后最后projector的部分再打回NPU不行么
有INT16模式么）
这个reshape，NPU也不行么hmmm
他这个场景下应该不会出现零尺寸的tensor吧
请问如何安装 mmdeploy ?
然后里面的setup.py没看懂怎么run的
有啥 zero shot 的检测网络吗？用于工业产线的长尾应用，例如这个月检测包裹，下个月检测自行车 [让我看看]
有没有先用算法过一遍再手调的
为啥二维码要自己做 微信的不好用么）
我在想这玩意技术难点在哪
不可能吧，用 buildroot 的会比较快吧
讲道理没有人优化一下clip吗，这clip看不到细节有点难受呀
为啥突然讨论这个严肃的问题，我爬楼看看
神经元数量够的话  下一步似不似是不是在网络里面添加主动获取奖励的机制  进行迭代 神经元数量够的话  下一步是不是在网络里面添加主动获取奖励的机制  进行迭代
ads能仿左手传输线嘛...一般得用CST，求解maxwell
mmcv-lite 一开始是给俺们组定制的，为啥 release 出去了。
mmcv有预编译whl，拉下来装了 就缺mmcv.runner 怕不是新版本又改名字了吧
我又不过手钱... 钱都是直接转给你们。 怎么是 agent 了呢。
openxlab 是什么情况？ 我在上面下载数据集，看起来也是上海实验室的
嘉立创 eda 不也是开源改的吗
农科院特供版本 吧？
你买了吗 他那个太虚了[旺柴]
群友想上作者列表么？ 想上咱就加。
请问用了 torch 的话算不算落地用了 cuda
豆爷没有回答过我的问题（ 也算做贡献了吗
被豆哥回答过问题，是不是属于参与测试和开发了
好奇，他怎么做到每天几篇文章的 机器人？
对了上次就想问，大老师这id
emmm，写之前没说约束么
友友们，有没有win端好用点的todo工具
不要的话我问问其他人，还以为你有资料
佬们，有啥目标跟踪网络使用的 gridsample op 吗？
佬们。ncnn的运行时开销大不，计算shape alloc这类动作
说的好，但是。。。。为啥没啥人用呢
现在npu 里面的simd单元一般是什么架构啊
就把langchain那玩儿直接模型化不行吗
话说我有一个问题，多个数据做推理，是batch推理快，还是用for循环batch1快？ 会有明显的速度差吗？
所以mnn最后是因为开发人员都去做llm而放弃了维护了吗
我就好奇这内部版是更快呢，还是bug更多呢
ray也可以做推理？
话说群友们有需要刀乐的嘛[捂脸]
AX什么时候支持pnnx
有大佬教一教用rtmo代码跑pose任务的吗
虫叔解决了服务器放modelzoo需求了么[奸笑]
用的wechaty 付费puppet么
可以手工测试一下 是不是有防火墙 或者wsl2和外面是啥桥接模式
群友们我有一个问题，我用opencv对一个mask做高斯模糊，很慢，有什么更快的实现吗，python上的
有啥推荐的吗？轻一点的 Transformer Detection
你们自己设计的么）外包是不是吃回扣了）
大模型啥时候才能反应快一些 感觉可以替换 be my eye
这不是正式版了吗，为何还要要自己编 emoji
太搞了，我就在论文里看见了一个这个表。有没有dict啊
插件链接发一下啊
嗯，custom pmic，有些奇怪的小功能 瓜佬觉得有啥救嘛）
不过爱芯不是有freertos的版本么
为什么我的orin nano 装完系统dp 没有输出呢 [捂脸]装了好几遍最后才发现是dp 没有输出
我开了选项也不好使，不知道咋用
你这咋是forWindows？
来问问大佬们这个咋回事
深度模型能整点啥好玩demo吗
opencompass如何调用自己的模型代码？
opencompass如何添加自定义模型？
现在端测llm推理用的都还是torch吗
sys msg mmdet应该也有排名把
这玩意真的有用吗 我用公司电脑安装这个插件的话还能用吗 sys msg 我用公司电脑安装这个插件的话会被调查吗
话说白座，现在国内有效果还行的能看图的llm吗？开源或者商用但能调api的也行
啊，啥模型的呀，我也想试试多模态的ocr
有没有比较简单的，做视频的工具？ sys msg
这个软件他的定位是什么呀？如果是pr那种定位的话，然觉也不值得呀
不会是上海ai lab的吧[捂脸]
类似之前的多塔吗 加一个ocr塔
用mmengine的东西里面怎么跑的多进程？怎么用的registry
有时候明明没在想 但是突然就跳出来告诉我 想通了 这是后台在运算吗 但是我感觉我的脑子没有多线程能力啊
白座那个项目后面支持llama2了嘛
这里为什么设置两边呢 两遍
还有和芯片一样大的散热片能不能压住 主要需求是需要评估用不用得着风扇 emoji
他只能禁止反编译，那我从cuda源码编译到别的平台他管得着么
大佬们，yolov5要怎么训才能训出8bit量化后掉3/4个点这种水平
LLM 的适配情况吗 sys msg LLM 的适配情况呢
llama.cpp就能解决的事儿，为啥还需要专门的大模型推理框架呢
RWKV 的模型架构还没稳定吧
话说现在有好用的全景分割模型吗？segment anything分割出来的感觉太碎了
不过dram内存和ssd的原理好像有点差异 不知道可么
1.5 你是不是少看了个.
话说对于超分这种模型，做量化效果好吗？
咦，这里 dsp 干嘛的
微信早就说有linux版了。不知道在哪
）Score board是怎么实现的[捂脸] 感觉还是跑不了DB
我有个问题啊，这些llm的test都是publicly available的 那后来的LLM是不是直接就学到了
是你传上去的不？ 空的你没传是吧
sensor还能怎么革命...
[图片] 啥是roqdmap sys msg 啥是roadmap
笑笑知道怎么用 LLM 做 function call 吗
问个小白的问题 mipi 需要train吗
昨天说的那个奇怪的噪点 感觉像ISP参数的问题 demosic参数问题么
说起来）上次问的，620Q上有木有啥现成的clip based的模型可以demo一下的） 最好是seg或者detection啥的
他的demo我跑不起来 有点奇怪的问题
pnnx过一遍看看能不能把einsum消掉？
你这个计算是怎么算的呀，看不懂 你要不找个啥trace一下给哥们弄成reshape啥的拼起来让哥们看懂一下下
compiler 写咋样了
我怎么感觉这给图片有问题
[让我看看]所以是 底层api 还是中间层
什么效果，这么严苛 开源那些东西不够用吗
）想请教NV GPU上的一个问题，我在kernel里面的很多参数，比如说thread block数量之类的，那driver是直接像变量一样套用，还是需要重新编译
就，你从API的层面上来说
应该是Reshape Transpose可以不用？
万能的群，有啥模型可以做类似的图像增强？
上次群里说要搞清楚 uboot atf bl1 啥的得买个啥书来着？
[发呆] kernel 怎么启动啥的，都不会
话说torch这么庞大的东西，他得有多少个专职的程序员呀
你见过验证说软件提供的pdump太多，导致验证进度很慢，所以要删掉一些测例的吗[旺柴]
所以是不是用int4就更扯了
数据量跟参数量匹配，大的没法直接炼出来，所以小的也不行？
[旺柴] token好像也是训练出来的吧
这么高大上的facexformer里面为啥要塞一个mtcnn
你们是不是在说tengine
risc-v 不是不要钱吗 为啥没人直接硬怼多核？ risc-v X 1024 [旺柴]
那种方式的已经学会了 现在是模型就是有好几G这种咋个搞[捂脸]
telechat 好用吧
天工大模型没有开放的api么
[旺柴]不过你有什么具体需求么 现在这个不是挺好的
的量化没做group？
ncnn 量化完了跑的时候是 一层一层解开跑 嘛
传统的cnn算法？难道装个MNN做端上训练？
主要是训练需要浮点运算吧 还要算损失函数 反向传播 端侧有点奢侈吧
能外部输入权重嘛（
水佬 有没有开源的 vlm画图模型啊
俩字只算一个token你咋不提了呢
水佬教我玩量化吧~~
现在代码能力强的模型是哪个呀 能解决动态规划问题的
这不是全带的意思么？ 啊，这是个选择题啊，没看出来
yolov8pose如何跑coco指标呢
yolov8pose没有coco指标怎么和mmpose模型对比呢
请问我在部署onnx推理时，为什么在同一个py文件中用build_task_processor分别初始化mmdet和mmpose的processor后会导致mmdet_processor.create_input时报错呢？
能问一下，mmpose  rtmpose dwpose他们之间什么关系
刚才还想问mmpose是不是类似于pytorch的框架
大佬们，我看tokenpose-s的参数量挺少的啊，Gflops也还行，为啥部署不用呢？是因为推理速度慢还是因为部署不友好（算子支持差）啊？
一个小问题，SDK的姿态估计示例可以传入物体的bbox，如果在多人场景下，只传特定的某个人的框，是不是忽略了其他人，类似于做单人姿态估计，速度快一点
大佬们可以帮忙看看这个数据配置文件吗
这个有训自己数据集的教程吗？
各位大佬请教，要是使用mmpose训练自己的数据集需要标注的时候，有什么推荐的标注工具吗？
请问是否有将mmpose部署到jetsin xavier nx的教程吗？
请问是否有将mmpose部署到jetson xavier nx的教程吗？
你这样问问题效率很低的，你说了半天到现在连你跑的哪个数据集都不说，让我们掐指一算靠猜么
请问为啥yolov8xpose-p6模型的时候，它还需要下载yolov8n.pt的权重呢。 感觉跑关键点检测的模型，也用不到yolov8n.pt这个权重文件
不知道我要是跑yolov8x-pose-p6模型，是不是要下载yolov8x.pt的权重文件
yolov8pose的map是怎么算的呢，感觉和coco指标的ap差别很大。
镜佬的one stage方案打算啥时候来个preview版本啊
各位大佬们yolopose出现这个问题是啥原因呀
佬们，mediapipe pose是不是速度最快的人体姿态识别的方案了
dw对比rtm有啥优势？除了精度高一丢丢？
dw用的就是rtm的模型，结构啥都一样，速度怎么能快呢
Flow模型是不是都比较少人研究啊[捂脸]
想请问下，bottomup模式下，跑的dekr那个方法，标注的目标框是还在训练中起作用吗？我手动设bbox = []是没法训练的，在想着标注的时候可以不检测框，只标关键点不？
mmpose调试coco的代码时加载数据很慢 这种情况各位有遇到过吗
想问一下调试速度是和底层gcc这些有关吗[捂脸]
是的，你用的是预训练权重吧
打扰大佬，还有一个mmpose的问题，我使用单人的视频，输出的json文件，好多frame里面有两组 key_points, box_score 都是 1，但是第一组的keypoint_score 比第二组高。为啥会输出两组呢？
大家有没有遇到类似的问题，会不会是论文造假了[捂脸]
你要检查一下为什么是0，它不应该是0
有大佬知道 怎么提取出 骨干网络 生成的特征图么？
有什么可视化工具么？
只用调用参数use_udp就行了嘛？
请问mmpose算法库 代码能改吗
看文档注册机制不太明白
兄弟们，比如说我在一个数据集上训练了一段时间得到了一个新的权重文件，然后我对这个数据集再加了点图片，想用那个权重文件继续训练新的数据集，使用resume还是修改配置文件里面的checkponit呀
就这种结果图能用mmpose跑出来吗 大佬们
你要不再对着代码理解一下吧 或者看看开放麦的录像
simcc被 RTMHead优化了 所以实现了 高的性能， 那么 rtmhead 在其他模型上会有更好的表现么？
嗯哼？ 所以您的意思是 SimCC 最后 实际上还是 一对反卷积最后才加的分类头 出的效果么？
emmm  就是 设计一个好的Pipeline？
眸佬，mmdetection能支持多数据集一起训练吗？
有用dinov2的原始预训练模型搞检测的吗？
大佬，这茴香豆怎么训练的？后面打算开源吗？
请问为什么那个demo会报错呢？
MMDeploy模型部署-Pytorch转ONNX. 目标检测模型出现'NoneType' object has no attribute 'copy' 问题，求解谢谢
请问豆哥，mmpose 有哪些keypoint loss，每种loss适用什么场景？
mmaction那种模型可以实现多人动作识别？
大佬们，我想用rtmpose 只检测画面最靠前的人，有啥好办法吗？
Mmaction2torchserve有问题吧？
但是看飞浆那边的Paddle 的生态项目里 有这种 mmpose有类似的实现嘛？
只追求精度，rtm是不是并非最优选择
请问一下，在mmpose里边怎么对数据集做采样处理呀？
行为检测模型如何转tensorrt？
有个问题哈，为啥onnx模型在jetson设备上部署，不是直接转成trt模型就行，而是要搞一大堆的mmdeploy的部署[疑问][疑问]
能不能把mmdeploy的算子直接搬到trt?
可以指定mmdeploy的算子？
yolopose关键点检测好像只能识别出 框和关键点 ，如何实现识别前后转动的角度呢
还是那个问题哈，onnx模型在jetson设备上部署，有好的方法直接转成trt模型，避免搞一大堆mmdeploy的部署吗[疑问][疑问]
如果没报错，该生成so的话，那就会在对应文件夹里面有吧
大佬们，怎么办，mmdet的onnx模型里的topk，这是tensorrt算子不支持吗
mmaction 中 Skeleton-based Spatio-Temporal Action Detection，有什么办法可以转变为tensorrt吗？
「JPMark: 「焕军：是的， trt 的 topk 必须要是个固定值。 mmdeploy 里有对应 rewrite 」 - - - - - - - - - - - - - - - 试了下Nvidia自己的polygraphy和convertmodel的转换，不work，老哥你说的rewrite在哪，能给个地址不」 ————————— https://github.com/open-mmlab/mmdeploy/blob/main/mmdeploy/pytorch/functions/topk.py
各位大佬，我想咨询一个问题，关于topdown与bottom up的。如果我纯输入一张已经识别到的目标图像（通过目标检测器识别裁剪操作，得到的图像中只一个目标），后面在关键点检测中，topdown模型能不能像bottomup模型那样直接使用，不用设置mmdet环节。
小白问个问题，MMdetection怎么能够使用继承torch.autograd.Function自定义的损失函数，总是报错TypeError: cannot pickle 'LossFun' object in <mmengine.hooks.ema_hook.EMAHook object at 0x7fc845e0d6a0>,求各位大佬能帮助一下
各位大佬， 咨询一个问题，模型训练时 batchsize 与 num-_work 两个参数，怎么根据gpu的型号调整哇
各位大佬，我想咨询一个模型训练的问题。我有一组Ms coco格式的关键点数据集，里面标注了66个关键点，但我只想用其中的30个关键点来训练模型，并且不考虑点之间的连接关系。这种情况能实现吗，如何修改配置文件
或者说，没有这个bbox_files文件夹，也能对模型进行训练吧？
我是用gpu跑的，3080 有什么优化的方法吗？
是不是需要cmake opencv才能用cuda？
大佬，可以分享一下 OneHand10K数据集吗[加油]
我用coco的配置训练出来的模型速度比body8的模型速度快正常吗？为啥会这样？
大佬们，我需要高精度的关键点检测算法，时间因素没有那么重要，朋友们有推荐的吗
大佬们  用simcc训练hrnet 占用gpu 为什么那么大
大佬，有个关于自定义数据集的问题请教一下。就是我有一组coco数据集，里面对每个对象标注66个关键点。我计划只用其中30个关键点来训练模型，请问有什么方法可以对这个做处理吗[皱眉]
大佬一个hrnet训练出来，一个epoch占七百多内存，正常吗 [囧]
对一下cuda 版本，检查cuda补丁包装了没
想请问一下rtmpose部署成功后，官方的API推理提供了单张图片示例，但如果想对视频进行推理，用cap循环读取帧图片再cv2读入img做检测速度很慢，有什么更好的处理方法嘛
要不你试试blazepose？
谁有关于遮挡优化的可视化对比图吗[合十][合十][合十]
突然想到，你直接crop不也应该一样能构造不少样本么？
问一下我这输出要咋改 难道再加个卷积 [疑问] hrnet输出固定的不是
各位大佬，我测试自己训练的shufflenetv2 模型，报如下错误。大佬们知道这是怎么回事哇
大佬们我在装MMPose的时候这个测试我做不出来，是咋回事
大佬们 mmdeploy的虚拟环境能和mmpose的环境用同一个么？
我想问一下 rtmpose 我先训练目标检查 可以不加关键点嘛 就是只有边框的 coco格式 没有关键点
要不大佬 你实验一下 拿coco数据集跑一下 也很快 我看看是不是我自己的问题
镜佬 RTMPose这个图里面的具体数据能 从哪里获取么？
请问我想在mmpose的训练中，我val的时候，评估两个模型，有什么修改方法吗
这里后面的log里的ap和这里显示的ap对不上啊
请问下mmdetection框架dino算法，backbone只有swin-l，有没有模型更小swin-t等配置文件？
大佬们，这个错误是什么原因啊，mmdeploy-runtime-gpu
有大佬知道怎么找到smpl-x模型中kinematic chains 的实现部分在哪儿吗
大佬们 有人知道yolo的txt格式数据的深度挖掘怎么做吗
请问一下  基于mmpose做的代码  准备提交code给会议，版权或者开源协议这块要怎么弄
大佬们，我跑yolov8 关键点检测报这种错 有人知道怎么解决吗
chunk预测模式，有推荐事例代码吗，想参考下
我的需求可能是预测的类别比较多，我在想分一级类别两级类别预测，减少prompt输入；还是现在输出，prompt输入比较长
话说bert这种语言模型是否也可以在推理时候简单外推下，让他在大概512范围内性能不会下降特别明显？ 有没有做nlp的佬 不要求外推很长，大概能够满足需求就行 256确实有点短了
可能得找一下有没有加rope的bert开源，但是何不换个语言模型呢，不一定要用bert吧
大佬们不使用预训练权重在哪调嘞
大佬们问一下 我就加了这个 训练没问题 但是预测时候 TypeError: __init__() got an unexpected keyword argument 'radius' why？
你用哪个脚本推理的？
[破涕为笑]你有git bash吗
不是说windows 和 linux 兼容吗
应该是这一部分吧
想把pose 在multi task的时候集成到vlm 里面，大佬们有啥建议吗？
大佬能给个链接吗？一直没找到
是和这个warning有关吗
同目录下的__init__.py 那里export 这个模块了吗
大佬们，我用自己的数据集（三千多张图片17关键点）用hourglass和hrnet训练（batch size8）不到二十轮精度就0.9了，这是为什么啊[苦涩]
不同类别作关键点检测，如果想先分类再关键点检测，是分开训练好还是联合训练好，不是太懂。如果分开训练(分类模型的数据集和关键点检测的数据集是否需要一致呢，因为感觉关键点检测效果先依赖于分类效果，如果关键点检测的测试集图片刚好出现在分类人物的训练集中，感觉不太行哇)
higherhrnet这种down_top的可以吧？  openpose这种？
请问大佬们，关键点检测模型比如rtmpose可以判断此关键点是否处于遮挡状态吗
是不是可以结合加机器学习的方法 比如svm异常检测
大佬们，我要往模型后面继续加模块咋办，比如卷积啥的，但是显示是list不让我操作，怎么变成tensor
这个onnx转成rknn后模型的输出是什么啊？
想问下大佬们，rtmpose预测的关检点得分为什么会是大于1的呢，这个得分的范围是怎样的呢
要是要查看模型的参数量是需要去自定义HOOK吗
但这样在创建模型时好像也不会用我修改的模型代码。还是我代码改错了
大佬们？rtmpose的onnx为啥转不成功啊？hardsigmoid怎么避免。官方什么版本转的
有大佬推荐一个比较好跑的backbone模型吗
想找一个现成代码
mmpose关于服饰关键点检测的算法模型在哪里呀
你这github链接 还是空的
想问问，rtmo和yoloxpose是不是共用一个backbone和neck，只是head不同
使用mmdepoly转onnx报错，是不是暂时还不支持呢？ rtmo
奇怪，我这边用 opset version 11 是可以的。你pytorch，onnx版本都是多少呀
请问mmpose，代码运行过程中数据是怎么一步一步传递的，想尝试改一下数据预处理的代码，但看的时候代码全是一块一块的，捋不明白，有啥方法吗/
想问问大佬用的板子是什么框架的？
请问把脸部、身体关键点模型，移植到手机端，模型轻量化需要做哪些工作，怎么调整呢？
mm新人看了一天的rtmo代码，相比yolox-pose，是不是在neck多少transformer的结构和在head上多了dcc模块，以及loss_mle mm菜鸡新人看了一天的rtmo代码，想问问相比yolox-pose，是不是在neck多少transformer的结构和在head上多了dcc模块，以及loss_mle
大佬们，想问一下，bottom up关键点检测 以你们复现的dekr为例，这个算法只能接受一类框吗？能不能接受多类框。他是可以接受多类框，然后根据不同框的名字，组织名字相同的关键点生成热图拿去训练是吗？
请问大佬如果我想自己标记一个 小规模的 video 2D关节点检测数据集 ，有没有什么比较好用自动化较高的工具呀？
这是优化的结果，为啥你还想多用点内存
我就调训练配置的线程数，那个pencv thread怎么调？
大佬，不行啊，这该怎么加速训练速度啊？188G的运行内存，怎么也用不到30G，加线程也没用
workers设太高是不是也没啥用 单卡训练的话
有大佬用过onnx的静态int8量化嘛
请问在哪里可以设置一下输出val的loss值呢？
loss非常低有啥问题吗
有大佬试过rtmo 转ncnn 有坑嘛？
sys msg 佬 RTMO 在predict的时候 为啥是直接调用的 YOLOPose 做的处理？
这一行之后 不都是在 检测和解码了么
正常不应该是用你设计的 forward_train来 编码特征图么
数据长啥样，方便的话可以来张图
有大佬了解吗，运用同一个rknn文件分别用c++和python推理（前处理都相同），推理出的结果有差异吗
大佬们为啥batchsize设置一样 [图片] 一个这么多 一个一百多 一个yolov3 一个efnv2 sys msg 一个主干网络 mbv2
哥，看看我这啥问题
[图片] 啥也没动，就是mmdection配置好的 怎么差距这么大 理论上mobienetv2还轻量化模型 batch size 都是32
大佬，训练rtmpose时batch大于100就会出现这个错误怎么回事啊？明明显卡内存还有足够的量使用 [图片]
我想想 训练数据能撑满bs100么
用小的bs训练完成过么？这个报错也可能是有数据集加载失败
各位大佬，我有一个关于特征点检测的问题想咨询下。我利用自己的车辆特征点数据训练了HRNET模型，用来提取车辆图像的特征点。我测试了下，发现预测的有些特征点在bbox外面，这种情况正常吗？ [图片] [图片] 例如像这种
debugpy怎么调ddp啊，佬会吗
兄弟们，3d骨架的数据有标注工具嘛？ emoji
各位大佬，topdown方法会在bbox外预测特征点的默认机制可以取消吗
大佬，mmpose有动作迁移的代码吗？
大佬detinferencer 用grounding dino 出来的结果 label 是数字，这个数字 是和text 用.分割一一对应的嘛？
请问，我用yolov5做自顶向下的检测中的nms换成diou_nms，为啥红色框中人物叠在一块的时候，还是一个框 [图片]
您好，请问这个我如果想要用比较新版本的模型去test 推理。 这个我该要怎么么写呢？
似乎不太行，好像还缺了些
那我自定义数据集要哪里修改类和个数嘞
诶如果你加metainfo训练能跑，为啥推理会跑不了
mmseg支持新的config，需要怎样修改
看看 是哪一步出错了是嘛，好[皱眉] 我试试
[图片] 训练rtmo的时候报错 有哪位大佬知道为啥嘛？
有没有大佬遇到过用rtmw-l转为tensorrt出来后，置信度大于1了，这是咋回事呢？mmpose和mmdeploy是最新的 [图片]
有没有兄弟说一下这个model_path在哪里[抱拳]
1.x的 generate box 的py 文件去哪里了
感觉你的验证集好像有点弱
这精度看着还行，rtmo用的哪个，RTMO-s？
大佬,rtmo是不是还没给出c++的前后处理代码?
百度云找了一个，这个靠谱吗？ 有大佬下载过human3.6m这个数据集吗？
各位大佬，我想了解一下metain元文件中的joint_weights与sigmas参数的大小是怎么定义设置的？是不是joint_weights越大，模型越专注训练这个点，sigmas越大是不是表示在这个点越难学？ [图片]
请问一下大佬，mmpose里有测试onnx推理速度的代码吗
请问，现在mmpose的inference可以使用mmtrack来进行检测框的追踪任务吗？
[图片] 飞哥我想问下在训练阶段 [图片] 这块是进行归一化、padding和bgr2rgb操作，那 [图片] pipeline这块是在哪完成的呢？ [图片] 飞哥我想问下在训练阶段
佬们，请问hrnet训练自定义的关键点数据集的时候，是不是要修改损失函数呀，因为之前hrnet的损失函数是根据真实标签和预测标签的置信度使用均方差计算得到的，如果用自己的标注的数据集，没有真实标签的置信度呢？咋办啊[流泪] 也不知道理解得对不对[困][流泪]
请问有修改过rtmpose网络架构的大佬吗～
不好意思我是小白[苦涩]，请问从源码安装的话，直接修改models/backbone/cspnext.py后，就能用config直接训练修改后的架构吗？还需要其他什么步骤吗？比如什么注册器之类的
或者可以提供代码学习吗（貌似脸皮有点厚了...[旺柴]
镜佬，啥时候出个好量化的架构啊[捂脸]
大佬们请教个修改网络架构的问题[社会社会] 我想修改models/backbone/cspnext.py里的网络架构，没有改类名称，但是改好架构之后好像没有被调用。  我试着在CSPNeXt类模块的前中后print语句，发现只有前后能打印，类中没有打印。  但如果类中有语法错误，是无法训练的。请问是我修改的文件位置不对吗？还是改完之后还需要什么操作才能确保被实例化和调用呢？
昂！确实有scope=mmdet，那请问要怎么改过来才直接调用models/backbone/cspnext.py里的呢？
我把rtmo的backbone换成mobilenetv2，试着只在coco上训，但总是训崩，这个有啥训练技巧么 是不是得先在yolopose上预训练
我取了8、16、32三个level的feature map，然后neck改成了PAFPN,融合后不输出stride=8的feature，这个应该没问题吧
有大佬教一教用rtmo代码跑pose任务的吗 有没有相关的学习视频
请教大佬们：将cspnext的标准卷积换成深度可分离卷积之后，batch=1也报错RuntimeError:  CUDA out of memory。 这是为什么呢？按理说换成深度可分离卷积模型计算复杂度变小了呀
视频超分网络 mmdeploy支持了吗
你看一下mmskeleton上次更新是啥时候
不过我是用的mmaction2，发现不太对，所以用论文进的源码，发现还是这样，所以有点疑问
请教一个问题，我想用mmpose重新训练一个checkpoint，resume之后，他的学习率还是初始的学习率，但是这个checkpoint的学习率应该是一个更小的值（用了余弦退火，这一轮的学习率和一开始的学习率不一样）。这种情况应该怎么办
请问下 mmpose有没有算法硬件适配相关的说明啊  比如在哪个移动端芯片上进行实测一类
有大佬了解理论方面的知识吗，原本的yolopose是检测人体17个关键点，现在本人的数据集不是人体中的17个关键点，是另外10个点。这应该是对原yolopose中的哪一部分进行改进了吗
镜佬，视频版本关键点模型，有后续跟踪+滤波示例吗？
对你课题的目标定义一下就可以了
这个输出不是有个384 512吗，改了inputsize这个要改吗
有大佬知道，rtmpose这个脸部模型，需要用哪个detection模型吗？
会不会是我用的权重太新了？ sys msg 会不会是我用的模型太新了
大佬们，C++实现fc的矩阵乘法有什么好方法吗？rtmpose的最后三个fc层我必须要把输入拉成一维才能在我设备上量化[捂脸]
感谢各位大佬 上个问题解决了[Salute] 有一个新问题 我用“wholebody”测一个没有人出现的片段[捂脸]，保存的json当中几乎所有的bbox_score得分都是1 ，这个1是什么意思呀？
你可以找下有没有对量化友好的位置编码方法
lmdeploy中cache-max-entry-count代表什么，和显存占用的关系是什么？
请问如果在其他项目训练好了目标检测器，想要和mmpose里的top-down方法一起用来推理，能够实现吗？有大佬试过吗？
代码里是对的 网页上面链接错了吧
首先你得看能不能检测到，其次我感觉再看能不能检测到关键点
那这种目标检测分类应该能弄出来吧
我在尝试学习使用mmpose自定义模型，自定义模型的主干网络的话，主干网络的输出需要是什么样的才能和颈接到一块啊
请问哪位大佬有deep image matting数据集，求分享[抱拳]
难道用gt训练出来的，实际检测的时候结果更差？
请问有朋友统计过Hourglass-52的参数量和FLOPs吗？能正常训练，但是统计参数量和FLOPs的命令跑不通
请问运行tools/test.py输出的可视化图像，一张图像上只能呈现单目标的结果吗？
我想的是不知道群友有没有人造过轮子把mmpose跑的结果改成标注 再去精修的 emoji
以及 它的效果能超过openface这种专业模型吗？
有人在用yolov8坐关键点检测吗
mmpose 除了能拿到关键点坐标信息还能拿到什么数据信息嘛
按照说明注释掉了这些东西是不是训练和测试的时候都不会进行rescore了？
请问一下各位大佬，有没有关键点检测数据增强的案例啊
请问有人用过SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation这篇网络的代码吗，想交流一下，谢谢 主要复现不出trainforscratch的性能
佬，你们用什么部署的
你是没有加自己数据吗 这种情况下的数据
拿学生的视频标注深度？然后再训练是嘛
验证集不需要同步增强吗，我不太懂，那这样的话增强训练集的同时验证集始终很小 我再查查 谢谢大佬
有大佬用过mmaction2嘛
[可怜]问一下，做金丝猴的姿态识别要识别的关键点有哪些？包括尾巴吗？还有流程是先标注关键点，然后弄出对应的.json文件做训练集和测试集，后面是进行目标检测和关键点检测就结束了吗？
请教大佬一个问题，mmaction2的demo_skeleton生成的视频为啥打不开
大佬们，mmcv和mmpose可以安装在深度学习的环境中吗？还是要单独建一个虚拟环境？
这个安装mmpose，显示‘git’不是内部或外部命令，也不是可运行的程序或批处理文件。这怎么弄？ [图片]
对物体进行关键点检测用yolo好了还是rtm pose好呢
大佬，git base这个命令窗口怎么打开我创建的虚拟环境呢？
大佬们，在虚拟环境中安装mmpose，已经下载了git，也配置好了环境，在anaconda的命令窗口使用git命令还是不行，这怎么弄？
你用的windows还是linux
那官网安装教程在虚拟环境中搞这个命令是干嘛的？新手不太懂[破涕为笑]
豆哥在我修改网络模块时候yolo.py中parse函数应该如何书写 网络卷积.yaml文件应如何修改
验证MMpose是否安装正确，怎么弄？
训练的关键点检测模型oks怎么看勒
mmpose里有哪些算法模型是transformer架构的？
mmpose有测试推理速度的工具吗
哈？ 你测pck了吗
为什么没有那个scalars.json文件呢 可视化那个
请问大佬们有在工程部署中，有多个pytorch模型使用多线程来同时推理？
请问大佬一个问题，Action Recognition可以是行为识别嘛
有人在mmdet3d群吗 可以拉我一下
rtmpose有没有使用纯onnxruntime部署的教程？
我就没有那个json文件呀，那是训练生成的吧
如果是aarch64，按照android的教程编译就可以了吧
请问在模型部署的时候，图像的预处理和结果的后处理都会被包含到模型里面么？
em 我之前是查官方api写的 可以搜搜看
可以讲讲mxnet吗
大佬们有同时基于不同的backends（如nvidia和amd）训练过大模型么
就想问问你们，把一个大模型基于模型并行或者其他的策略，同时基于nvidia的gpu和其他的device（如华为、AMD）之类的，多机多卡，有这么训练过的么
关于网络冻结 冻结第2层 然后第1层还能传回梯度吗
好奇还有没有人用paddle了
有云服务器吗  如果比nv便宜 直接用
感觉mmdet里的voc 评估器是不是有点问题 总是卡住不动了
眸佬刚刚说今天放出来的paper是什么
我少关注哪个公众号了[捂脸]
眸佬是哪一个repo呢？[呲牙]
mmdet是不是还没支持tinyperson和visdrone
也是个好问题，把task定义好
对于segformer模型有一个问题，PatchEmbed最后是一个layernorm，然后接下来计算的block中的一个也是layernorm，相当于是两个layernorm挨着这是为什么，没想明白为什么这么设计[捂脸]，官方的和mmseg都是这样的
我主要是看到mmEngine集成了它 就想问问效果咋样
请问下，mmseg 训练的时候图片大小如果是 64*64，那它预测的时候是怎么分割更大的图片的呀。 我看 unet 在 drive 数据集上是 64*64，但是我在推理的时候，使用更大的图片也能完成分割，想请教下是怎么做到的
是因为我用的是 unet 吗，所以可以 size 无关
问一下 你怎么看openmmlab
但是这个不是数据增强后数据的可视化嘛 我是想知道具体经过了哪些数据增强
群里有人做过交互式目标检测吗？或者有一定了解吗？
类似sam那种交互吗
想问一下，怎么设置train_cfg可以让前n个epoch训练dataset1，后m个epoch训练dataset2呢
帮问，mmdeploy c++怎么设置占用资源，比如线程数
mmagic有finetune的指导文档吗
有没有可能你会转成coco也就会处理数据了
请问一下如果想修改mmengine里面的sd 我应该改哪个文件
我看mmengine的文本编码器和stablediffusion是封装到一起的，改怎么去修改文本的编码器
Openmmlab以后会有强化学习的库吗
请问各位。mmdet怎么使用fp16训练，用了ampoptinwarpper，参考了mmengine。显存不降反增
插队问一句 大佬们知道有哪些好的polyline检测器么
想问下各位佬，注册openreview的时候多久能激活账号啊[捂脸]3号前就不能修改名单了
可以对照demo代码写吧
我看其他数据集的label的彩图 都是8位的 我自己做的png就是24位的  他们是特意保存为8位的应该是
佬们，请问如何在mmdet里面支持新的标注数据集
请问mmseg里面有没有膨胀推理的功能
大佬们~mmpretrain里，swin-transformer-v2，官方权重转换mmpretrain的代码有嘛[旺柴]
要是写pr稿，标题我昨晚都想好了：  你真的超过llama2 了吗？
请教一个问题，模型在forward的时候可以过去runner的当前迭代次数吗？还是必须重写一个forward把runner作为参数传过去呢
不懂就问：POE里就有chatGPT3.5等一众语言模型， 可以替代ChatGPT网页了？
mmdet什么时候支持Mapillary Vistas数据集啊
第一次写分布式训练代码 想问问这种每个epoch开始loss会有断崖式的下降正常不 只保存了rank 0的log
有没有比swin transformer更好训练更高效的backbone推荐呀
不过这几个好像不行吧
佬们，有链接吗[抱拳]
应该不会吧 因为我问 3他说他自己不是 4
朋友们请教个问题, 这个python简单调了十几个库, 怎么分析都是那些tensor 或者变量占了多少显存
detclip 请问大家有见过开源版本么？
不知道上次是咋安装上的
请问这儿指定rk3588后后端自动将onnx转rknn去run吗？可不可以让我自己提供rknn模型呢[旺柴]
求助各位带佬，eva-vit模型，用torch.onnx.export导出onnx，结果出现这么多文件，而且onnx只有360k。咋肥事捏~ 求助各位带佬，eva-vit模型，用torch.onnx.export导出onnx，结果出现这么多文件，而且onnx文件大小只有360k。咋肥事捏~
可能是输入图像的尺寸有点小吧
能问下大家有办法修改list[dict]里面的元素嘛
能问下大家有办法修改配置文件里的list[dict]里面的元素嘛 那只能多加个参数写代码里了[苦涩]
不是改config吗？ [捂脸]
不知道诶，你可以问问豆哥
这个部署推理 会默认检测cuda吗
上几千个核心的超算应该能快点吧
看MMEngine源码？
有大佬了解吗，真的可以比得上gpt4吗
有大佬玩过 retnet 吗？真的可以取代 transformer 吗
不懂就问，那未来的发展方向是洗数据吗？[让我看看]
如果效果大差不差，研究这么多网络干嘛呢[捂脸]
我导师在做多任务的时候，其中一个任务，就直接用一个很简单的 MLP 网络去做分类，所以我有些疑惑，感觉这个网络比较简单，我当时就在想为什么不用VIT或者更复杂高级的网络呢
是用的统一内存api吗 可以超量申请
最近整了一些大模型的教程有佬想提前体验一下的吗[让我看看]有算力支持
请问下，mmdet的test.py，如何让其导出推理结果而非metrics？是不是必须自己写一个runner和TestLoop类的run方法才可以？
看下本地 cuda 版本和 torch-cuda 小版本是不是一样。
请问模型推理的时候，预处理支持一次处理一个batch么？我看现在还是循环每张图片做预处理？ [图片]
请教一下佬 目前在做一个医学影像处理的小工作 在做语义分析的时候想自己做一套数据集 但是niftii这种数据是个三维的感觉 似乎不同于以往的jpg这种一张一张的 该怎么标注得到mask呢[捂脸]
能问下有办法可视化DETR系列的encoder和decoder的特征图嘛
大佬们，mask2former里面，这些不需要配置嘛[嘿哈] 怎么默认知道是Mask2FormerTransformerDecoder
请教大佬们，对于开集检测，有那种“输入对于一种实体的描述，然后输出一个或者多个对应这种实体的检测框”的视觉任务吗
是不是类似detGPT那种
我直接用的mmcv里实现的，应该也没有什么问题？
能问下各位佬vscode可以直接调试多卡的代码吗
data_preprocessor中的crop_size？
data_preprocessor哪来的crop_size
想问下这个群有搞体系结构的大佬吗，有两个课堂作业题不会。。求助大佬们帮帮忙，可以有偿（。 [图片] emoji
想问下佬们，mmdet里面有好用的测fps的工具嘛？ sys msg
佬们~mmseg 1.x ，如果要支持新的config，就是支持config里跳转的话。只需要把包引用进来，然后把字符串的符号去掉嘛？还有什么地方需要动呢? [图片]
大佬们，在尝试新config的时候，报错这个。 绿色框，是打印输出的dataset_cfg的内容 [皱眉]可能是什么问题呢
[图片] 看起来是数据集 config的 问题嘛？
能再说说看 meta device吗
大家有遇到过这种多卡训练的时候一个epoch只log一次的嘛，况且训练得特别快，感觉应该没有训练到1 sys msg 大家有遇到过这种多卡训练的时候一个epoch只log一次的嘛，况且训练得特别快，感觉应该没有训练到12个epoch
估计是在train循环里面的bug？
请问图文对的dataset是不是无法下载本地 （无法hf cli下载
nuscenes数据集在火山云上，本地如何读取他的pkl文件，有大佬能指点一二吗？
大佬们好，请问一下，一般来说，数据加载是目标检测模型训练时间的瓶颈吗
想问一下 我做的目标检测数据集，用的mmdet的数据集标签格式，怎么使用Coco的标准来计算mAP 在mmdet里没有找到相应的类[破涕为笑]，看了一下自己写评估代码的话好麻烦呀[捂脸]，有没有什么办法
有好兄弟跑过llava的mmbench的evaluation么，我就用官方的权重复现，发现分数比论文里还高了，大家有试过的么
是不是得先在yolopose上预训练
agentlego的工具大部分也要用huggingface的模型，（因为网络问题下载不了）请问离线模型应该存放在哪个位置[玫瑰][发抖]
有朋友调用过 whisper v3嘛  无论是 api 还是本地方式 （想找一个封装好的接口作视频转录
有个事想请教大佬们。俺在变化检测这种小领域 应用了一种全新训练模式和寄生模块，用93.74的精度跟base的90.26拉开了差距，并打败了小领域内SOTA同一个backbone下91.86的精度。这种开山之作的工作有能力发顶会吗
请教下各位大佬 我最近在做一个工服识别项目，用分类算法，目标工服是上下装 纯蓝色，是一个二分类问题 是/不是目标工服。在制作数据集的时候有个疑问:只准备两类数据集——蓝色和其他；还是准备红橙黄绿蓝靛紫等十多种衣服类别数据来训练模型
这不是商业项目里的问题么…
话说白座 最后rage这个架构师折腾了多久弄出来的
为啥这cogvlm的specialist模型才比generalist模型高了一点点 [图片]
mmseg里，如果测试数据集的尺度差异很大，512*512也有  1024*1024也有 1w*1w也有 那么这个值可以让 图片是多大就设置多大嘛 [图片] 把Resize去掉嘛？
豆哥，如果一张图是1024*1024输入，Resize=(512,512)，test_cfg=dict(mode='slide',crop_size=(512,512)，stride=（384，384））是不是没用呢
想请教一个问题，我在用mmaction2训练的时候，GPU温度快超过正常范围了，并且功耗也超过了正常的范围，请问这样该怎么解决？显卡我用的是两块2080TI
你是不是有个图没发出来
「MMSeg-Dominic23331: 想请教一个问题，我在用mmaction2训练的时候，GPU温度快超过正常范围了，并且功耗也超过了正常的范围，请问这样该怎么解决？显卡我用的是两块2080TI」 ————————— 57 度还行啊？ 没到 80 都不用担心。
「MMSeg-Dominic23331: 想请教一个问题，我在用mmaction2训练的时候，GPU温度快超过正常范围了，并且功耗也超过了正常的范围，请问这样该怎么解决？显卡我用的是两块2080TI」 ————————— 看截图， fan 就 57% ，你没发温度图。  如果温度超过 80 才可能是问题。
「MMSeg-Dominic23331: 「焕军：「MMSeg-Dominic23331: 想请教一个问题，我在用mmaction2训练的时候，GPU温度快超过正常范围了，并且功耗也超过了正常的范围，请问这样该怎么解决？显卡我用的是两块2080TI」 ————————— 看截图， fan 就 57% ，你没发温度图。  如果温度超过 80 才可能是问题。」 - - - - - - - - - - - - - - - 温度已经超80了」 ————————— 那检查主板，扩展新散热。 最简单的，找个大风扇。
看之前的内核能不能启动
豆哥是焕军老师的爱好吗，还是InternLM团队的分配项目啊[捂脸]
想问问大家有没有遇到过WSL2不能克隆GitHub仓库的情况 网上的解决方案对我完全没有 而且我能ping上Google但是ping不上GitHub
请问pytest在测试的时候，如果通过了测试样例，但是还希望把print的内容输出到终端，应该怎么写呢？
Openmmlab是不是有个AI 实战营，有提供算力的吗？
有没有大佬熟悉mmdetection里的AutoAugment机制 想知道它具体是如何运作的
我们学校的不知道发了几个
如何设定bs 达到效率最高点？ 默认开启了compile的情况
如何更多提高训练速度呢 也想知道 就简单做实验跑跑 相同策略测一下精度而已
这个不晓得还有没有
想试试mindformers吗[旺柴]
别讲应用，讲点关键点怎么算就行。
vscode是升级了cpp版本导致的吗
请教大佬们。为什么 github issues 的api  获得的里面有pull啊 不全是 issues
我想知道想设计mm这种架构 或者更简单些的架构需要学什么
请问：梯度累计 对loss的影响是什么呢 除了可以平滑 防止局部最优
这样吗，主要要做代码迁移，想看看论文[皱眉]
谢谢佬，mmdeploy是能转换就能部署还是转换大于部署呀[发抖]
想请教下各位大佬，我自定义了一个backbone，也注册了，在train和val时都没有什么问题。在test的时候报错说我的backbone is not in the mmseg::model registry。想问下这个问题如何解决呀[捂脸]
请问下mmcv的new config，自定义字段可以使用class作为value吗
mmdetection中有没有训练好的，KITTI数据集中行人检测的模型
yoloworld精度会比dino差很多吗 好奇
问一下mmdetection的，现在给他打补丁，用哪个branch？
这个应该是看opset的版本吧
可以model输出那里打个断点？
各位佬这里是什么意思？，我没太明白，就算是加了lazy import，解析时不也是需要torch去解析sgd吗？
路径错了，没读进来数据把
问个问题，板子只支持8bit整型推理，输入是语音类的连续变量怎么处理啊，有大佬知道怎么处理这类问题吗
现在刚上手xtuner  不知道需要改哪些地方 emoji
请问下为啥mmdet里的有些模型只需要跑1x 2x个epoch 一般不都是2 3百嘛
好的 谢谢 我看有些检测模型需要epoch好几百 有些只需要2x 这有啥说法嘛
mmdet里有什么好办法，让一个模型同时在不同的验证集上进行测试吗[捂脸]
Concatedateset不是把几个数据集拼成一个大的数据集吗？可以分别给出在不同测试集上的测试结果吗？[捂脸]
佬们，这个分析analyze_results.py怎么调用gpu进行推理啊，好慢
这样啊，为啥处理4000多个结果要三个小时这么鬼久的[破涕为笑]
mmdeploy是什么项目，lmdeploy吗
豆哥，mmdeploy是什么
Cosine Annealing 和 Warm Restart 是不是目前比较推荐的最佳实践？（非大模型）
我在想 这种正负样本的思路 是不是可以用在生成文本上 感觉好像有用 又想不到怎么用
我最近也要做一个意图识别 大佬求带
请问 mim install 比较慢是因为啥呀, mim 会使用pip 的国内镜像吗
mim 好像可以加上这个 feature ?
看看单个数据的大小还有numwork的设置
你分别看一下 batch=2 与 4 的
老哥们，mmdeploy可以用tta吗？
是要 小图推理 再合并嘛
slurm不好调试吧
我看好多都是用torchserve部署所以有这个疑问
