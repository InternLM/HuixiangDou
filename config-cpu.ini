[feature_store]
# `feature_store.py` use this throttle to distinct `good_questions` and `bad_questions`
reject_throttle = -1.0
# text2vec model, support local relative path, huggingface repo and URL.
# for example:
#  "maidalun1020/bce-embedding-base_v1"
#  "BAAI/bge-m3"
#  "https://api.siliconflow.cn/v1/embeddings"
embedding_model_path = "https://api.siliconflow.cn/v1/embeddings"

# reranker model, support list:
#  "maidalun1020/bce-reranker-base_v1"
#  "BAAI/bge-reranker-v2-minicpm-layerwise"
#  "https://api.siliconflow.cn/v1/rerank"
reranker_model_path = "https://api.siliconflow.cn/v1/rerank"

# if using `siliconcloud` API as `embedding_model_path` or `reranker_model_path`, give the token
api_token = ""
api_rpm = 800
api_tpm = 40000
work_dir = "workdir"

[web_search]
engine = "serper"
# web search engine support ddgs and serper
# For ddgs, see https://pypi.org/project/duckduckgo-search
# For serper, check https://serper.dev/api-key to get a free API key
serper_x_api_key = "YOUR-API-KEY-HERE"
domain_partial_order = ["openai.com", "pytorch.org", "readthedocs.io", "nvidia.com", "stackoverflow.com", "juejin.cn", "zhuanlan.zhihu.com", "www.cnblogs.com"]
save_dir = "logs/web_search_result"

[llm]
[llm.server]
# remote LLM service configuration
# support "gpt", "kimi", "deepseek", "zhipuai", "step", "internlm", "xi-api" and "alles-apin"
# support "siliconcloud", see https://siliconflow.cn/zh-cn/siliconcloud
# xi-api and alles-apin is chinese gpt proxy
# for internlm, see https://internlm.intern-ai.org.cn/api/document

remote_type = "siliconcloud"
remote_api_key = ""
# max text length for remote LLM.
# use 128000 for kimi, 192000 for gpt/xi-api, 16000 for deepseek, 128000 for zhipuai, 40000 for internlm2
remote_llm_max_text_length = 40000
# openai API model type, support model list:
# "auto" for kimi. To save money, we auto select model name by prompt length.
# "auto" for step to save money, see https://platform.stepfun.com/
# "gpt-4-0613" for gpt/xi-api,
# "deepseek-chat" for deepseek,
# "glm-4" for zhipuai,
# "gpt-4-1106-preview" for alles-apin or OpenAOE
# "internlm2-latest" for internlm
# for example "alibaba/Qwen1.5-110B-Chat", see https://siliconflow.readme.io/reference/chat-completions-1
remote_llm_model = "alibaba/Qwen1.5-110B-Chat"
# request per minute
rpm = 500


[worker]
# enable web search or not
enable_web_search = 1
# enable search enhancement or not
enable_sg_search = 0
# enable coreference resolution in `PreprocNode`
enable_cr = 0
save_path = "logs/work.txt"

[worker.time]
enable = 0
start = "00:00:00"
end = "23:59:59"
has_weekday = 1

[frontend]
# chat group assistant type, support "lark_group", "wechat_personal", "wechat_wkteam" and "none"
# for "lark_group", open https://open.feishu.cn/document/home/introduction-to-custom-app-development/self-built-application-development-process to create one
# for "wechat_personal", read ./docs/add_wechat_group_zh.md to setup gateway
# for "wkteam", see https://wkteam.cn/
type = "none"

# for "lark", it is chat group webhook url, send reply to group, for example "https://open.feishu.cn/open-apis/bot/v2/hook/xxxxxxxxxxxxxxx"
# for "lark_group", it is the url to fetch chat group message, for example "http://101.133.161.20:6666/fetch", `101.133.161.20` is your own public IPv4 addr
# for "wechat_personal", it is useless
webhook_url = "https://open.feishu.cn/open-apis/bot/v2/hook/xxxxxxxxxxxxxxx"

# when a new group chat message is received, should it be processed immediately or wait for 18 seconds in case the user hasn't finished speaking?
# support "immediate"
message_process_policy = "immediate"

[frontend.lark_group]
# "lark_group" configuration examples, use your own app_id and secret !!!
app_id = "cli_a53a34dcb778500e"
app_secret = "2ajhg1ixSvlNm1bJkH4tJhPfTCsGGHT1"
encrypt_key = "abc"
verification_token = "def"

[frontend.wechat_personal]
# "wechat_personal" listen port
bind_port = 9527

[frontend.wechat_wkteam]
# wechat message callback server ip
callback_ip = "101.133.161.11"
callback_port = 9528

# public redis config
redis_host = "101.133.161.11"
redis_port = "6380"
redis_passwd = "hxd123"

# wkteam
account = ""
password = ""
# !!! `proxy` is very import parameter, it's your account location
# 1：北京 2：天津 3：上海 4：重庆 5：河北
# 6：山西 7：江苏 8：浙江 9：安徽 10：福建
# 11：江西 12：山东 13：河南 14：湖北 15：湖南
# 16：广东 17：海南 18：四川 20：陕西
# bad proxy would cause account deactivation !!!
proxy = -1

# save dir
dir = "wkteam"

# 群号和介绍
# 茴香豆相关
[frontend.wechat_wkteam.43925126702]
name = "茴香豆群（大暑）"
introduction = "github https://github.com/InternLM/HuixiangDou 用户体验群"

[frontend.wechat_wkteam.44546611710]
name = "茴香豆群（立夏）"
introduction = "github https://github.com/InternLM/HuixiangDou 用户体验群"

[frontend.wechat_wkteam.38720590618]
name = "茴香豆群（惊蛰）"
introduction = "github https://github.com/InternLM/HuixiangDou 用户体验群"

[frontend.wechat_wkteam.48437885473]
name = "茴香豆群（谷雨）"
introduction = "github https://github.com/InternLM/HuixiangDou 用户体验群"

[frontend.wechat_wkteam.34744063953]
name = "茴香豆群（雨水）"
introduction = "github https://github.com/InternLM/HuixiangDou 用户体验群"

# github.com/tencent/ncnn contributors
[frontend.wechat_wkteam.18356748488]
name = "卷卷群"
introduction = "ncnn contributors group"