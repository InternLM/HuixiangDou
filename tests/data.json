[
    {
      "question": "[æ‚è„¸]å‡ å¹´å‰ç”¨å¤§æ’çš„å·¥ä¸šç›¸æœºï¼Œè¿˜å¾—ç”¨ä»–ä»¬çš„SDKï¼Œè¿˜æ²¡armçš„ç‰ˆæœ¬ï¼Œæœ€åè®©ä»–ä»¬ç¼–äº†ä¸€ä¸ªarmçš„ç”¨",
      "part": " å¿«é€Ÿä¸Šæ‰‹ éƒ¨ç½² internlm #### è·å– internlm æ¨¡å‹  \n#### ä½¿ç”¨ turbomind æ¨ç†  \n> **note**<br />\n> turbomind åœ¨ä½¿ç”¨ fp16 ç²¾åº¦æ¨ç† internlm-7b æ¨¡å‹æ—¶ï¼Œæ˜¾å­˜å¼€é”€è‡³å°‘éœ€è¦ 15.7gã€‚å»ºè®®ä½¿ç”¨ 3090, v100ï¼Œa100ç­‰å‹å·çš„æ˜¾å¡ã€‚<br />\n> å…³é—­æ˜¾å¡çš„ ecc å¯ä»¥è…¾å‡º 10% æ˜¾å­˜ï¼Œæ‰§è¡Œ `sudo nvidia-smi --ecc-config=0` é‡å¯ç³»ç»Ÿç”Ÿæ•ˆã€‚  \n> **note**<br />\n> ä½¿ç”¨ tensor å¹¶å‘å¯ä»¥åˆ©ç”¨å¤šå¼  gpu è¿›è¡Œæ¨ç†ã€‚åœ¨ `chat` æ—¶æ·»åŠ å‚æ•° `--tp=<num_gpu>` å¯ä»¥å¯åŠ¨è¿è¡Œæ—¶ tpã€‚  \n#### å¯åŠ¨ gradio server  \n!  \n#### é€šè¿‡ restful api éƒ¨ç½²æœåŠ¡  \nä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š  \nä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œæ–¹å¼ä¸æ¨ç†æœåŠ¡è¿›è¡Œå¯¹è¯ï¼š  \nä¹Ÿå¯ä»¥é€šè¿‡ webui æ–¹å¼æ¥å¯¹è¯ï¼š  \næ›´å¤šè¯¦æƒ…å¯ä»¥æŸ¥é˜… restful_api.mdã€‚  \n#### é€šè¿‡å®¹å™¨éƒ¨ç½²æ¨ç†æœåŠ¡  \nä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š  \nä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œæ–¹å¼ä¸æ¨ç†æœåŠ¡è¿›è¡Œå¯¹è¯ï¼š  \nä¹Ÿå¯ä»¥é€šè¿‡ webui æ–¹å¼æ¥å¯¹è¯ï¼š  \nå…¶ä»–æ¨¡å‹çš„éƒ¨ç½²æ–¹å¼ï¼Œæ¯”å¦‚ llamaï¼Œllama-2ï¼Œvicunaç­‰ç­‰ï¼Œè¯·å‚è€ƒè¿™é‡Œ",
      "full": "<div align=\"center\">\n  <img src=\"resources/lmdeploy-logo.png\" width=\"450\"/>\n\n[![docs](https://img.shields.io/badge/docs-latest-blue)](https://lmdeploy-zh-cn.readthedocs.io/zh_CN/latest/)\n[![badge](https://github.com/InternLM/lmdeploy/workflows/lint/badge.svg)](https://github.com/InternLM/lmdeploy/actions)\n[![PyPI](https://img.shields.io/pypi/v/lmdeploy)](https://pypi.org/project/lmdeploy)\n[![license](https://img.shields.io/github/license/InternLM/lmdeploy.svg)](https://github.com/InternLM/lmdeploy/tree/main/LICENSE)\n[![issue resolution](https://img.shields.io/github/issues-closed-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)\n[![open issues](https://img.shields.io/github/issues-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)\n\n[English](README.md) | ç®€ä½“ä¸­æ–‡\n\n</div>\n\n<p align=\"center\">\n    ğŸ‘‹ join us on <a href=\"https://twitter.com/intern_lm\" target=\"_blank\">Twitter</a>, <a href=\"https://discord.gg/xa29JuW87d\" target=\"_blank\">Discord</a> and <a href=\"https://r.vansin.top/?r=internwx\" target=\"_blank\">WeChat</a>\n</p>\n\n______________________________________________________________________\n\n## æ›´æ–° ğŸ‰\n\n- \\[2023/08\\] TurboMind æ”¯æŒ Qwen-7Bï¼ŒåŠ¨æ€NTK-RoPEç¼©æ”¾ï¼ŒåŠ¨æ€logNç¼©æ”¾\n- \\[2023/08\\] TurboMind æ”¯æŒ Windows (tp=1)\n- \\[2023/08\\] TurboMind æ”¯æŒ 4-bit æ¨ç†ï¼Œé€Ÿåº¦æ˜¯ FP16 çš„ 2.4 å€ï¼Œæ˜¯ç›®å‰æœ€å¿«çš„å¼€æºå®ç°ğŸš€ã€‚éƒ¨ç½²æ–¹å¼è¯·çœ‹[è¿™é‡Œ](./docs/zh_cn/w4a16.md)\n- \\[2023/08\\] LMDeploy å¼€é€šäº† [HuggingFace Hub](https://huggingface.co/lmdeploy) ï¼Œæä¾›å¼€ç®±å³ç”¨çš„ 4-bit æ¨¡å‹\n- \\[2023/08\\] LMDeploy æ”¯æŒä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•è¿›è¡Œ 4-bit é‡åŒ–\n- \\[2023/07\\] TurboMind æ”¯æŒä½¿ç”¨ GQA çš„ Llama-2 70B æ¨¡å‹\n- \\[2023/07\\] TurboMind æ”¯æŒ Llama-2 7B/13B æ¨¡å‹\n- \\[2023/07\\] TurboMind æ”¯æŒ InternLM çš„ Tensor Parallel æ¨ç†\n\n______________________________________________________________________\n\n## ç®€ä»‹\n\nLMDeploy ç”± [MMDeploy](https://github.com/open-mmlab/mmdeploy) å’Œ [MMRazor](https://github.com/open-mmlab/mmrazor) å›¢é˜Ÿè”åˆå¼€å‘ï¼Œæ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚\nè¿™ä¸ªå¼ºå¤§çš„å·¥å…·ç®±æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š\n\n- **é«˜æ•ˆæ¨ç†å¼•æ“ TurboMind**ï¼šåŸºäº [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)ï¼Œæˆ‘ä»¬å®ç°äº†é«˜æ•ˆæ¨ç†å¼•æ“ TurboMindï¼Œæ”¯æŒ InternLMã€LLaMAã€vicunaç­‰æ¨¡å‹åœ¨ NVIDIA GPU ä¸Šçš„æ¨ç†ã€‚\n\n- **äº¤äº’æ¨ç†æ–¹å¼**ï¼šé€šè¿‡ç¼“å­˜å¤šè½®å¯¹è¯è¿‡ç¨‹ä¸­ attention çš„ k/vï¼Œè®°ä½å¯¹è¯å†å²ï¼Œä»è€Œé¿å…é‡å¤å¤„ç†å†å²ä¼šè¯ã€‚\n\n- **å¤š GPU éƒ¨ç½²å’Œé‡åŒ–**ï¼šæˆ‘ä»¬æä¾›äº†å…¨é¢çš„æ¨¡å‹éƒ¨ç½²å’Œé‡åŒ–æ”¯æŒï¼Œå·²åœ¨ä¸åŒè§„æ¨¡ä¸Šå®ŒæˆéªŒè¯ã€‚\n\n- **persistent batch æ¨ç†**ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ‰§è¡Œæ•ˆç‡ã€‚\n\n  ![PersistentBatchInference](https://github.com/InternLM/lmdeploy/assets/67539920/e3876167-0671-44fc-ac52-5a0f9382493e)\n\n## æ”¯æŒçš„æ¨¡å‹\n\n`LMDeploy` æ”¯æŒ `TurboMind` å’Œ `Pytorch` ä¸¤ç§æ¨ç†åç«¯\n\n### TurboMind\n\n> **Note**<br />\n> W4A16 æ¨ç†éœ€è¦ Ampere åŠä»¥ä¸Šæ¶æ„çš„ Nvidia GPU\n\n|   æ¨¡å‹   | æ¨¡å‹å¹¶è¡Œ | FP16 | KV INT8 | W4A16 | W8A8 |\n| :------: | :------: | :--: | :-----: | :---: | :--: |\n|  Llama   |   Yes    | Yes  |   Yes   |  Yes  |  No  |\n|  Llama2  |   Yes    | Yes  |   Yes   |  Yes  |  No  |\n| InternLM |   Yes    | Yes  |   Yes   |  Yes  |  No  |\n\n### Pytorch\n\n|   æ¨¡å‹   | æ¨¡å‹å¹¶è¡Œ | FP16 | KV INT8 | W4A16 | W8A8 |\n| :------: | :------: | :--: | :-----: | :---: | :--: |\n|  Llama   |   Yes    | Yes  |   No    |  No   |  No  |\n|  Llama2  |   Yes    | Yes  |   No    |  No   |  No  |\n| InternLM |   Yes    | Yes  |   No    |  No   |  No  |\n\n## æ€§èƒ½\n\n**åœºæ™¯ä¸€**: å›ºå®šçš„è¾“å…¥ã€è¾“å‡ºtokenæ•°ï¼ˆ1,2048ï¼‰ï¼Œæµ‹è¯• output token throughput\n\n**åœºæ™¯äºŒ**: ä½¿ç”¨çœŸå®æ•°æ®ï¼Œæµ‹è¯• request throughput\n\næµ‹è¯•é…ç½®ï¼šLLaMA-7B, NVIDIA A100(80G)\n\nTurboMind çš„ output token throughput è¶…è¿‡ 2000 token/s, æ•´ä½“æ¯” DeepSpeed æå‡çº¦ 5% - 15%ï¼Œæ¯” huggingface transformers æå‡ 2.3 å€\nåœ¨ request throughput æŒ‡æ ‡ä¸Šï¼ŒTurboMind çš„æ•ˆç‡æ¯” vLLM é«˜ 30%\n\n![benchmark](https://github.com/InternLM/lmdeploy/assets/4560679/7775c518-608e-4e5b-be73-7645a444e774)\n\n## å¿«é€Ÿä¸Šæ‰‹\n\n### å®‰è£…\n\nä½¿ç”¨ pip ( python 3.8+) å®‰è£… LMDeployï¼Œæˆ–è€…[æºç å®‰è£…](./docs/zh_cn/build.md)\n\n```shell\npip install lmdeploy\n```\n\n### éƒ¨ç½² InternLM\n\n#### è·å– InternLM æ¨¡å‹\n\n```shell\n# 1. ä¸‹è½½ InternLM æ¨¡å‹\n\n# Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone https://huggingface.co/internlm/internlm-chat-7b /path/to/internlm-chat-7b\n\n# if you want to clone without large files â€“ just their pointers\n# prepend your git clone with the following env var:\nGIT_LFS_SKIP_SMUDGE=1\n\n# 2. è½¬æ¢ä¸º trubomind è¦æ±‚çš„æ ¼å¼ã€‚é»˜è®¤å­˜æ”¾è·¯å¾„ä¸º ./workspace\npython3 -m lmdeploy.serve.turbomind.deploy internlm-chat-7b /path/to/internlm-chat-7b\n\n```\n\n#### ä½¿ç”¨ turbomind æ¨ç†\n\n```shell\npython3 -m lmdeploy.turbomind.chat ./workspace\n```\n\n> **Note**<br />\n> turbomind åœ¨ä½¿ç”¨ FP16 ç²¾åº¦æ¨ç† InternLM-7B æ¨¡å‹æ—¶ï¼Œæ˜¾å­˜å¼€é”€è‡³å°‘éœ€è¦ 15.7Gã€‚å»ºè®®ä½¿ç”¨ 3090, V100ï¼ŒA100ç­‰å‹å·çš„æ˜¾å¡ã€‚<br />\n> å…³é—­æ˜¾å¡çš„ ECC å¯ä»¥è…¾å‡º 10% æ˜¾å­˜ï¼Œæ‰§è¡Œ `sudo nvidia-smi --ecc-config=0` é‡å¯ç³»ç»Ÿç”Ÿæ•ˆã€‚\n\n> **Note**<br />\n> ä½¿ç”¨ Tensor å¹¶å‘å¯ä»¥åˆ©ç”¨å¤šå¼  GPU è¿›è¡Œæ¨ç†ã€‚åœ¨ `chat` æ—¶æ·»åŠ å‚æ•° `--tp=<num_gpu>` å¯ä»¥å¯åŠ¨è¿è¡Œæ—¶ TPã€‚\n\n#### å¯åŠ¨ gradio server\n\n```shell\npython3 -m lmdeploy.serve.gradio.app ./workspace\n```\n\n![](https://github.com/InternLM/lmdeploy/assets/67539920/08d1e6f2-3767-44d5-8654-c85767cec2ab)\n\n#### é€šè¿‡ Restful API éƒ¨ç½²æœåŠ¡\n\nä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š\n\n```shell\npython3 -m lmdeploy.serve.openai.api_server ./workspace server_ip server_port --instance_num 32 --tp 1\n```\n\nä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œæ–¹å¼ä¸æ¨ç†æœåŠ¡è¿›è¡Œå¯¹è¯ï¼š\n\n```shell\n# restful_api_url is what printed in api_server.py, e.g. http://localhost:23333\npython -m lmdeploy.serve.openai.api_client restful_api_url\n```\n\nä¹Ÿå¯ä»¥é€šè¿‡ WebUI æ–¹å¼æ¥å¯¹è¯ï¼š\n\n```shell\n# restful_api_url is what printed in api_server.py, e.g. http://localhost:23333\n# server_ip and server_port here are for gradio ui\n# example: python -m lmdeploy.serve.gradio.app http://localhost:23333 localhost 6006 --restful_api True\npython -m lmdeploy.serve.gradio.app restful_api_url server_ip --restful_api True\n```\n\næ›´å¤šè¯¦æƒ…å¯ä»¥æŸ¥é˜… [restful_api.md](docs/zh_cn/restful_api.md)ã€‚\n\n#### é€šè¿‡å®¹å™¨éƒ¨ç½²æ¨ç†æœåŠ¡\n\nä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š\n\n```shell\nbash workspace/service_docker_up.sh\n```\n\nä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œæ–¹å¼ä¸æ¨ç†æœåŠ¡è¿›è¡Œå¯¹è¯ï¼š\n\n```shell\npython3 -m lmdeploy.serve.client {server_ip_addresss}:33337\n```\n\nä¹Ÿå¯ä»¥é€šè¿‡ WebUI æ–¹å¼æ¥å¯¹è¯ï¼š\n\n```shell\npython3 -m lmdeploy.serve.gradio.app {server_ip_addresss}:33337\n```\n\nå…¶ä»–æ¨¡å‹çš„éƒ¨ç½²æ–¹å¼ï¼Œæ¯”å¦‚ LLaMAï¼ŒLLaMA-2ï¼Œvicunaç­‰ç­‰ï¼Œè¯·å‚è€ƒ[è¿™é‡Œ](docs/zh_cn/serving.md)\n\n### åŸºäº PyTorch çš„æ¨ç†\n\nä½ å¿…é¡»ç¡®ä¿ç¯å¢ƒä¸­æœ‰å®‰è£… deepspeedï¼š\n\n```\npip install deepspeed\n```\n\n#### å•ä¸ª GPU\n\n```shell\npython3 -m lmdeploy.pytorch.chat $NAME_OR_PATH_TO_HF_MODEL\\\n    --max_new_tokens 64 \\\n    --temperture 0.8 \\\n    --top_p 0.95 \\\n    --seed 0\n```\n\n#### ä½¿ç”¨ DeepSpeed å®ç°å¼ é‡å¹¶è¡Œ\n\n```shell\ndeepspeed --module --num_gpus 2 lmdeploy.pytorch.chat \\\n    $NAME_OR_PATH_TO_HF_MODEL \\\n    --max_new_tokens 64 \\\n    --temperture 0.8 \\\n    --top_p 0.95 \\\n    --seed 0\n```\n\n## é‡åŒ–éƒ¨ç½²\n\n### Step 1. è·å–é‡åŒ–å‚æ•°\n\né¦–å…ˆï¼Œæ‰§è¡Œé‡åŒ–è„šæœ¬ï¼Œè·å–é‡åŒ–å‚æ•°\n\n> æ‰§è¡Œåï¼Œé‡åŒ–éœ€è¦çš„å„ç§å‚æ•°ä¼šå­˜æ”¾åœ¨ $WORK_DIR ä¸­; æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸­ä¼šç”¨åˆ°\n\n```\n\npython3 -m lmdeploy.lite.apis.calibrate \\\n  --model $HF_MODEL \\\n  --calib_dataset 'c4' \\             # æ ¡å‡†æ•°æ®é›†ï¼Œæ”¯æŒ c4, ptb, wikitext2, pileval\n  --calib_samples 128 \\              # æ ¡å‡†é›†çš„æ ·æœ¬æ•°ï¼Œå¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå¯ä»¥é€‚å½“è°ƒå°\n  --calib_seqlen 2048 \\              # å•æ¡çš„æ–‡æœ¬é•¿åº¦ï¼Œå¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå¯ä»¥é€‚å½“è°ƒå°\n  --work_dir $WORK_DIR \\             # ä¿å­˜ Pytorch æ ¼å¼é‡åŒ–ç»Ÿè®¡å‚æ•°å’Œé‡åŒ–åæƒé‡çš„æ–‡ä»¶å¤¹\n```\n\n### Step 2. å®é™…é‡åŒ–æ¨¡å‹\n\nç›®å‰æ”¯æŒå¯¹æƒé‡çš„ INT4 é‡åŒ–å’Œ KV Cache çš„ INT8 é‡åŒ–ï¼Œæ ¹æ®éœ€æ±‚æ‰§è¡Œå¯¹åº”è„šæœ¬å³å¯\n\n#### æƒé‡ INT4 é‡åŒ–\n\nLMDeploy ä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•å¯¹æ¨¡å‹æƒé‡è¿›è¡Œé‡åŒ–\n\n> éœ€è¦è¾“å…¥ç¬¬ä¸€æ­¥çš„ \\`$WORK_DIR\\`\\` ï¼Œé‡åŒ–åçš„æƒé‡ä¹Ÿä¼šå­˜åœ¨è¿™ä¸ªæ–‡ä»¶å¤¹ä¸­\n\n```\npython3 -m lmdeploy.lite.apis.auto_awq \\\n  --model $HF_MODEL \\\n  --w_bits 4 \\                       # æƒé‡é‡åŒ–çš„ bit æ•°\n  --w_group_size 128 \\               # æƒé‡é‡åŒ–åˆ†ç»„ç»Ÿè®¡å°ºå¯¸\n  --work_dir $WORK_DIR \\             # Step 1 ä¿å­˜é‡åŒ–å‚æ•°çš„ç›®å½•\n```\n\n[ç‚¹å‡»è¿™é‡Œ](./docs/zh_cn/w4a16.md) æŸ¥çœ‹ weight int4 ç”¨æ³•æµ‹è¯•ç»“æœã€‚\n\n#### KV Cache INT8 é‡åŒ–\n\n[ç‚¹å‡»è¿™é‡Œ](./docs/zh_cn/kv_int8.md) æŸ¥çœ‹ kv int8 ä½¿ç”¨æ–¹æ³•ã€å®ç°å…¬å¼å’Œæµ‹è¯•ç»“æœã€‚\n\n> **Warning**<br />\n> é‡åŒ–éƒ¨ç½²ä¸æ”¯æŒè¿è¡Œæ—¶ Tensor å¹¶å‘ã€‚å¦‚æœå¸Œæœ›ä½¿ç”¨ Tensor å¹¶å‘ï¼Œéœ€è¦åœ¨ deploy æ—¶é…ç½® tp å‚æ•°ã€‚\n\n## è´¡çŒ®æŒ‡å—\n\næˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ LMDeploy æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚è¯·å‚è€ƒ[è´¡çŒ®æŒ‡å—](.github/CONTRIBUTING.md)æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚\n\n## è‡´è°¢\n\n- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)\n- [llm-awq](https://github.com/mit-han-lab/llm-awq)\n\n## License\n\nè¯¥é¡¹ç›®é‡‡ç”¨ [Apache 2.0 å¼€æºè®¸å¯è¯](LICENSE)ã€‚\n"
    }
]
