[feature_store]
reject_throttle = 767.0
# text2vec model path, support local relative path and huggingface model format
# model_path = "/path/to/your/text2vec-large-chinese"
model_path = "GanymedeNil/text2vec-large-chinese"
work_dir = "workdir"

[web_search]
# open https://serper.dev/api-key to get a free API key
x_api_key = "${YOUR-API-KEY}"
domain_partial_order = ["openai.com", "pytorch.org", "readthedocs.io", "nvidia.com", "stackoverflow.com", "juejin.cn", "zhuanlan.zhihu.com", "www.cnblogs.com"]
save_dir = "logs/web_search_result"

[llm]
enable_local = 1
enable_remote = 1
# hybrid llm service address
client_url = "http://127.0.0.1:8888/inference"

[llm.server]
# local LLM configuration
# support "internlm2-7B", "internlm2-20B" and "internlm2-70B"
local_llm_path = "/internlm/ampere_7b_v1_7_0"
local_llm_max_text_length = 16000
local_llm_bind_port = 8888

# remote LLM service configuration
# support any python3 openai interface, such as "gpt", "kimi" and so on
remote_type = "kimi"
remote_api_key = "${YOUR-API-KEY}"
# max text length for remote LLM. for example, use 128000 for kimi, 192000 for gpt
remote_llm_max_text_length = 128000
# openai model type. use "moonshot-v1-128k" for kimi, "gpt-4" for gpt
remote_llm_model = "moonshot-v1-128k"

[worker]
enable_sg_search = 1
save_path = "logs/work.txt"

[worker.time]
start = "00:00:00"
end = "23:59:59"
has_weekday = 1

[sg_search]
binary_src_path = "/usr/local/bin/src"
src_access_token = "${YOUR-SRC-ACCESS-TOKEN}"

# add your repo here, we just take opencompass and lmdeploy as example
[sg_search.opencompass]
github_repo_id = "open-compass/opencompass"
introduction = "用于评测大型语言模型（LLM）. 它提供了完整的开源可复现的评测框架，支持大语言模型、多模态模型的一站式评测，基于分布式技术，对大参数量模型亦能实现高效评测。评测方向汇总为知识、语言、理解、推理、考试五大能力维度，整合集纳了超过70个评测数据集，合计提供了超过40万个模型评测问题，并提供长文本、安全、代码3类大模型特色技术能力评测。"
# introduction = "For evaluating Large Language Models (LLMs). It provides a fully open-source, reproducible evaluation framework, supporting one-stop evaluation for large language models and multimodal models. Based on distributed technology, it can efficiently evaluate models with a large number of parameters. The evaluation directions are summarized in five capability dimensions: knowledge, language, understanding, reasoning, and examination. It integrates and collects more than 70 evaluation datasets, providing in total over 400,000 model evaluation questions. Additionally, it offers evaluations for three types of capabilities specific to large models: long text, security, and coding."

[sg_search.lmdeploy]
github_repo_id = "internlm/lmdeploy"
introduction = "lmdeploy 是一个用于压缩、部署和服务 LLM（Large Language Model）的工具包。是一个服务端场景下，transformer 结构 LLM 部署工具，支持 GPU 服务端部署，速度有保障，支持 Tensor Parallel，多并发优化，功能全面，包括模型转换、缓存历史会话的 cache feature 等. 它还提供了 WebUI、命令行和 gRPC 客户端接入。"
# introduction = "lmdeploy is a toolkit for compressing, deploying, and servicing Large Language Models (LLMs). It is a deployment tool for transformer-structured LLMs in server-side scenarios, supporting GPU server-side deployment, ensuring speed, and supporting Tensor Parallel along with optimizations for multiple concurrent processes. It offers comprehensive features including model conversion, cache features for caching historical sessions and more. Additionally, it provides access via WebUI, command line, and gRPC clients."

[frontend]
# chat group type, support "lark" and "none"
# open https://open.feishu.cn/document/client-docs/bot-v3/add-custom-bot to add lark bot
type = "none"
# chat group webhook url, send reply to group
webhook_url = "https://open.feishu.cn/open-apis/bot/v2/hook/xxxxxxxxxxxxxxx"
