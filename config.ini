[feature_store]
reject_throttle = 767.0
# text2vec model path, support local relative path and huggingface model format
model_path = "../models/text2vec-large-chinese"
work_dir = "workdir"

[web_search]
# check https://serper.dev/api-key to get a free API key
x_api_key = "aa3da0cd69c5a2df7c0b664dc8a4c118de532405"
domain_partial_order = ["openai.com", "pytorch.org", "readthedocs.io", "nvidia.com", "stackoverflow.com", "juejin.cn", "zhuanlan.zhihu.com", "www.cnblogs.com"]
save_dir = "logs/web_search_result"

[llm]
enable_local = 1
enable_remote = 0
# hybrid llm service address
# client_url = "http://10.140.24.142:39999/inference"
client_url = "http://10.140.24.142:39999/inference"

[llm.server]
# local LLM configuration
local_llm_path = "/internlm/ampere_7b_v1_7_0"
local_llm_max_text_length = 16000

# remote LLM service configuration
# support any python3 openai interface, such as "gpt", "kimi" and so on
remote_type = "kimi"
remote_api_key = ""
# max text length for remote LLM. for example, use 128000 for kimi, 192000 for gpt
remote_llm_max_text_length = 128000
# openai model type. use "moonshot-v1-128k" for kimi, "gpt-4" for gpt
remote_llm_model = "moonshot-v1-128k"
bind_port = 8888

[worker]
enable_sg_search = 0
save_path = "logs/work.txt"

[worker.time]
start = "00:00:00"
end = "23:59:59"
has_weekday = 1

[sg_search]
binary_src_path = "/usr/local/bin/src"
src_access_token = "sgp_636f79ad2075640f_3ef2a135579615403e29b88d4402f1e6183ad347"

[frontend]
# chat group type, support "lark" and "none"
# check https://open.feishu.cn/document/client-docs/bot-v3/add-custom-bot to add bot
type = "none"
# char group webhook url, send reply to group
webhook_url = "https://open.feishu.cn/open-apis/bot/v2/hook/7a5d3d98-fdfd-40f8-b8de-851cb7e81e5c"
