<div align="center">
  <img src="resource/logo_blue.svg" width="550px"/>

<small> ç®€ä½“ä¸­æ–‡ | [English](README.md) </small>

[![GitHub license](https://img.shields.io/badge/license-BSD--3--Clause-brightgreen.svg?style=plastic)](./LICENSE)
[![pypi](https://img.shields.io/badge/install-PyPI-green.svg?style=plastic)](https://pypi.org/project/huixiangdou/)
![CI](https://img.shields.io/github/actions/workflow/status/internml/huixiangdou/lint.yml?branch=master&style=plastic)

</div>

â€œèŒ´é¦™è±†â€æ˜¯ä¸€ä¸ªåŸºäº LLM çš„é¢†åŸŸçŸ¥è¯†åŠ©æ‰‹ã€‚ç‰¹ç‚¹ï¼š

1. åº”å¯¹ç¾¤èŠè¿™ç±»å¤æ‚åœºæ™¯ï¼Œè§£ç­”ç”¨æˆ·é—®é¢˜çš„åŒæ—¶ï¼Œä¸ä¼šæ¶ˆæ¯æ³›æ»¥
2. æå‡ºä¸€å¥—è§£ç­”æŠ€æœ¯é—®é¢˜çš„ç®—æ³• pipeline
3. éƒ¨ç½²æˆæœ¬ä½ï¼Œåªéœ€è¦ LLM æ¨¡å‹æ»¡è¶³ 4 ä¸ª trait å³å¯è§£ç­”å¤§éƒ¨åˆ†ç”¨æˆ·é—®é¢˜ï¼Œè§[æŠ€æœ¯æŠ¥å‘Š](./resource/HuixiangDou.pdf)

æŸ¥çœ‹[èŒ´é¦™è±†å·²è¿è¡Œåœ¨å“ªäº›åœºæ™¯](./huixiangdou-inside.md)ï¼ŒåŠ å…¥[å¾®ä¿¡ç¾¤](./resource/figures/wechat.jpg)ä½“éªŒæœ€æ–°ç‰ˆæ•ˆæœã€‚

# ğŸ“¦ ç¡¬ä»¶è¦æ±‚

ä»¥ä¸‹æ˜¯è¿è¡ŒèŒ´é¦™è±†çš„ç¡¬ä»¶éœ€æ±‚ã€‚å»ºè®®éµå¾ªéƒ¨ç½²æµç¨‹ï¼Œä»åŸºç¡€ç‰ˆå¼€å§‹ï¼Œé€æ¸ä½“éªŒé«˜çº§ç‰¹æ€§ã€‚

|  ç‰ˆæœ¬  | GPUæ˜¾å­˜éœ€æ±‚ |                            æè¿°                            |                             Linux ç³»ç»Ÿå·²éªŒè¯è®¾å¤‡                              |
| :----: | :---------: | :--------------------------------------------------------: | :---------------------------------------------------------------------------: |
| åŸºç¡€ç‰ˆ |    22GB     |            èƒ½å›ç­”é¢†åŸŸçŸ¥è¯†çš„åŸºç¡€é—®é¢˜ï¼Œé›¶æˆæœ¬è¿è¡Œ            | ![](https://img.shields.io/badge/3090%2024G-passed-blue?style=for-the-badge)  |
| é«˜çº§ç‰ˆ |    40GB     |               èƒ½å¤Ÿå›ç­”æºç çº§é—®é¢˜ï¼Œé›¶æˆæœ¬è¿è¡Œ               | ![](https://img.shields.io/badge/A100%2080G-passed-blue?style=for-the-badge)  |
| é­”æ”¹ç‰ˆ |     4GB     | ç”¨ openai API æ›¿ä»£æœ¬åœ° LLMï¼Œå¤„ç†æºç çº§é—®é¢˜ã€‚éœ€è¦å¼€å‘èƒ½åŠ›ï¼Œè¿è¡Œéœ€è¦è´¹ç”¨ | ![](https://img.shields.io/badge/1660ti%206G-passed-blue?style=for-the-badge) |

# ğŸ”¥ è¿è¡Œ

æˆ‘ä»¬å°†ä»¥ lmdeploy & mmpose ä¸ºä¾‹ï¼Œä»‹ç»å¦‚ä½•æŠŠçŸ¥è¯†åŠ©æ‰‹éƒ¨ç½²åˆ°é£ä¹¦ç¾¤

## STEP1. å»ºç«‹è¯é¢˜ç‰¹å¾åº“

å¤åˆ¶ä¸‹é¢æ‰€æœ‰å‘½ä»¤ï¼ˆåŒ…å« '#' ç¬¦å·ï¼‰æ‰§è¡Œã€‚

```shell
# ä¸‹è½½ repo
git clone https://github.com/internlm/huixiangdou --depth=1 && cd huixiangdou

# ä¸‹è½½èŠå¤©è¯é¢˜
mkdir repodir
git clone https://github.com/open-mmlab/mmpose --depth=1 repodir/mmpose
git clone https://github.com/internlm/lmdeploy --depth=1 repodir/lmdeploy

# å»ºç«‹ç‰¹å¾åº“
mkdir workdir # åˆ›å»ºå·¥ä½œç›®å½•
python3 -m pip install -r requirements.txt # å®‰è£…ä¾èµ–ï¼Œè‹¥ python3.11 åˆ™éœ€è¦ `conda install conda-forge::faiss-gpu`
python3 -m huixiangdou.service.feature_store # æŠŠ repodir çš„ç‰¹å¾ä¿å­˜åˆ° workdir
```

é¦–æ¬¡è¿è¡Œå°†è‡ªåŠ¨ä¸‹è½½é…ç½®ä¸­çš„ [text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)ã€‚è€ƒè™‘åˆ°ä¸åŒåœ°åŒº huggingface è¿æ¥é—®é¢˜ï¼Œå»ºè®®å…ˆæ‰‹åŠ¨ä¸‹è½½åˆ°æœ¬åœ°ï¼Œç„¶ååœ¨ `config.ini` è®¾ç½®æ¨¡å‹è·¯å¾„ã€‚ä¾‹å¦‚ï¼š

```shell
# config.ini
[feature_store]
..
model_path = "/path/to/text2vec-large-chinese"
```

è¿è¡Œç»“æŸåï¼ŒèŒ´é¦™è±†èƒ½å¤ŸåŒºåˆ†åº”è¯¥å¤„ç†å“ªäº›ç”¨æˆ·è¯é¢˜ï¼Œå“ªäº›é—²èŠåº”è¯¥æ‹’ç»ã€‚è¯·ç¼–è¾‘ [good_questions](./resource/good_questions.json) å’Œ [bad_questions](./resource/bad_questions.json)ï¼Œå°è¯•è‡ªå·±çš„é¢†åŸŸçŸ¥è¯†ï¼ˆåŒ»ç–—ï¼Œé‡‘èï¼Œç”µåŠ›ç­‰ï¼‰ã€‚

```shell
# æ¥å—æŠ€æœ¯è¯é¢˜
process query: mmdeploy ç°åœ¨æ”¯æŒ mmtrack æ¨¡å‹è½¬æ¢äº†ä¹ˆ
process query: æœ‰å•¥ä¸­æ–‡çš„ text to speech æ¨¡å‹å—?
# æ‹’ç»é—²èŠ
reject query: ä»Šå¤©ä¸­åˆåƒä»€ä¹ˆï¼Ÿ
reject query: èŒ´é¦™è±†æ˜¯æ€ä¹ˆåšçš„
```

## STEP2. è¿è¡ŒåŸºç¡€ç‰ˆæŠ€æœ¯åŠ©æ‰‹

**é…ç½®å…è´¹ TOKEN**

èŒ´é¦™è±†ä½¿ç”¨äº†æœç´¢å¼•æ“ï¼Œç‚¹å‡» [Serper å®˜ç½‘](https://serper.dev/api-key)è·å–é™é¢ TOKENï¼Œå¡«å…¥ `config.ini`

```shell
# config.ini
..
[web_search]
x_api_key = "${YOUR-X-API-KEY}"
..
```

**æµ‹è¯•é—®ç­”æ•ˆæœ**

è¯·ä¿è¯ GPU æ˜¾å­˜è¶…è¿‡ 22GBï¼ˆå¦‚ 3090 åŠä»¥ä¸Šï¼‰ï¼Œè‹¥æ˜¾å­˜è¾ƒä½è¯·æŒ‰ FAQ ä¿®æ”¹ã€‚

é¦–æ¬¡è¿è¡Œå°†è‡ªåŠ¨ä¸‹è½½é…ç½®ä¸­çš„ [internlm2-chat-7b](https://huggingface.co/internlm/internlm2-chat-7b)ï¼Œè¯·ä¿è¯ç½‘ç»œç•…é€šã€‚

- **é docker ç”¨æˆ·**ã€‚å¦‚æœä½ **ä¸**ä½¿ç”¨ docker ç¯å¢ƒï¼Œå¯ä»¥ä¸€æ¬¡å¯åŠ¨æ‰€æœ‰æœåŠ¡ã€‚

  ```shell
  # standalone
  python3 -m huixiangdou.main --standalone
  ..
  ErrorCode.SUCCESS,
  Query: è¯·æ•™ä¸‹è§†é¢‘æµæ£€æµ‹ è·³å¸§  é€ æˆæ¡†ä¸€é—ªä¸€é—ªçš„  æœ‰å¥½çš„ä¼˜åŒ–åŠæ³•å—
  Reply:
  1. å¸§ç‡æ§åˆ¶å’Œè·³å¸§ç­–ç•¥æ˜¯ä¼˜åŒ–è§†é¢‘æµæ£€æµ‹æ€§èƒ½çš„å…³é”®ï¼Œä½†éœ€è¦æ³¨æ„è·³å¸§å¯¹æ£€æµ‹ç»“æœçš„å½±å“ã€‚
  2. å¤šçº¿ç¨‹å¤„ç†å’Œç¼“å­˜æœºåˆ¶å¯ä»¥æé«˜æ£€æµ‹æ•ˆç‡ï¼Œä½†éœ€è¦æ³¨æ„æ£€æµ‹ç»“æœçš„ç¨³å®šæ€§ã€‚
  3. ä½¿ç”¨æ»‘åŠ¨çª—å£çš„æ–¹å¼å¯ä»¥å‡å°‘è·³å¸§å’Œç¼“å­˜å¯¹æ£€æµ‹ç»“æœçš„å½±å“ã€‚
  ```

- **docker ç”¨æˆ·**ã€‚å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ dockerï¼Œ`HuixiangDou` çš„ Hybrid LLM Service éœ€è¦åˆ†ç¦»éƒ¨ç½²ã€‚

  ```shell
  # é¦–å…ˆå¯åŠ¨ LLM æœåŠ¡ï¼Œç›‘å¬ 8888 ç«¯å£
  python3 -m huixiangdou.service.llm_server_hybrid
  ..
  ======== Running on http://0.0.0.0:8888 ========
  (Press CTRL+C to quit)
  ```

  ç„¶åè¿è¡Œæ–°å®¹å™¨ï¼ŒæŠŠå®¿ä¸»æœºçš„ IP (æ³¨æ„ä¸æ˜¯ docker å®¹å™¨å†…çš„ IP) é…ç½®è¿› `config.ini`ï¼Œè¿è¡Œ

  ```shell
  # config.ini
  [llm]
  ..
  client_url = "http://10.140.24.142:9999/inference" # ä¸¾ä¾‹ï¼Œè¿™é‡Œéœ€è¦ä½ æ¢æˆå®¿ä¸»æœº IP

  # æ‰§è¡Œ main
  python3 -m huixiangdou.main
  ..
  ErrorCode.SUCCESS,
  Query: è¯·æ•™ä¸‹è§†é¢‘æµæ£€æµ‹..
  ```

## STEP3.é›†æˆåˆ°é£ä¹¦\[å¯é€‰\]

ç‚¹å‡»[åˆ›å»ºé£ä¹¦è‡ªå®šä¹‰æœºå™¨äºº](https://open.feishu.cn/document/client-docs/bot-v3/add-custom-bot)ï¼Œè·å–å›è°ƒ WEBHOOK_URLï¼Œå¡«å†™åˆ° config.ini

```shell
# config.ini
..
[frontend]
type = "lark"
webhook_url = "${YOUR-LARK-WEBHOOK-URL}"
```

è¿è¡Œã€‚ç»“æŸåï¼ŒæŠ€æœ¯åŠ©æ‰‹çš„ç­”å¤å°†å‘é€åˆ°é£ä¹¦ç¾¤ã€‚

```shell
python3 -m huixiangdou.main --standalone # é docker ç”¨æˆ·
python3 -m huixiangdou.main # docker ç”¨æˆ·
```

<img src="./resource/figures/lark-example.png" width="400">

å¦‚æœè¿˜éœ€è¦ä»é£ä¹¦ç¾¤è¯»å–ç¾¤èŠæ¶ˆæ¯ï¼Œè§[é£ä¹¦å¼€å‘è€…å¹¿åœº-æ·»åŠ åº”ç”¨èƒ½åŠ›-æœºå™¨äºº](https://open.feishu.cn/app?lang=zh-CN)ã€‚

## STEP4.é«˜çº§ç‰ˆ\[å¯é€‰\]

åŸºç¡€ç‰ˆå¯èƒ½æ•ˆæœä¸ä½³ï¼Œå¯å¼€å¯ä»¥ä¸‹ç‰¹æ€§æ¥æå‡æ•ˆæœã€‚é…ç½®æ¨¡æ¿è¯·å‚ç…§ [config-advanced.ini](./config-advanced.ini)

1. ä½¿ç”¨æ›´é«˜ç²¾åº¦ local LLM

   æŠŠ config.ini ä¸­çš„`llm.local` æ¨¡å‹è°ƒæ•´ä¸º `internlm2-20B`ã€‚
   æ­¤é€‰é¡¹æ•ˆæœæ˜¾è‘—ï¼Œä½†éœ€è¦æ›´å¤§çš„ GPU æ˜¾å­˜ã€‚

2. Hybrid LLM Service

   å¯¹äºæ”¯æŒ [openai](https://pypi.org/project/openai/) æ¥å£çš„ LLM æœåŠ¡ï¼ŒèŒ´é¦™è±†å¯ä»¥å‘æŒ¥å®ƒçš„ Long Context èƒ½åŠ›ã€‚
   ä»¥ [kimi](https://platform.moonshot.cn/) ä¸ºä¾‹ï¼Œä»¥ä¸‹æ˜¯ `config.ini` é…ç½®ç¤ºä¾‹ï¼š

   ```shell
   # config.ini
   [llm]
   enable_local = 1
   enable_remote = 1
   ..
   [llm.server]
   ..
   # open https://platform.moonshot.cn/
   remote_type = "kimi"
   remote_api_key = "YOUR-KIMI-API-KEY"
   remote_llm_max_text_length = 128000
   remote_llm_model = "moonshot-v1-128k"
   ```

   æˆ‘ä»¬åŒæ ·æ”¯æŒ chatgpt APIã€‚æ³¨æ„æ­¤ç‰¹æ€§ä¼šå¢åŠ å“åº”è€—æ—¶å’Œè¿è¡Œæˆæœ¬ã€‚

3. repo æœç´¢å¢å¼º

   æ­¤ç‰¹æ€§é€‚åˆå¤„ç†ç–‘éš¾é—®é¢˜ï¼Œéœ€è¦åŸºç¡€å¼€å‘èƒ½åŠ›è°ƒæ•´ promptã€‚

   - ç‚¹å‡» [sourcegraph-account-access](https://sourcegraph.com/users/tpoisonooo/settings/tokens) è·å– token

     ```shell
     # open https://github.com/sourcegraph/src-cli#installation
     sudo curl -L https://sourcegraph.com/.api/src-cli/src_linux_amd64 -o /usr/local/bin/src && chmod +x /usr/local/bin/src

     # å¼€å¯ sg æœç´¢ï¼Œå¹¶ä¸”æŠŠ token å¡«å…¥ config.ini
     [worker]
     enable_sg_search = 1 # first enable sg search
     ..
     [sg_search]
     ..
     src_access_token = "${YOUR_ACCESS_TOKEN}"
     ```

   - ç¼–è¾‘ repo çš„åå­—å’Œç®€ä»‹ï¼Œæˆ‘ä»¬ä»¥ opencompass ä¸ºä¾‹

     ```shell
     # config.ini
     # add your repo here, we just take opencompass and lmdeploy as example
     [sg_search.opencompass]
     github_repo_id = "open-compass/opencompass"
     introduction = "ç”¨äºè¯„æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰.."
     ```

   - ä½¿ç”¨ `python3 -m huixiangdou.service.sg_search` å•æµ‹ï¼Œè¿”å›å†…å®¹åº”åŒ…å« opencompass æºç å’Œæ–‡æ¡£

     ```shell
     python3 -m huixiangdou.service.sg_search
     ..
     "filepath": "opencompass/datasets/longbench/longbench_trivia_qa.py",
     "content": "from datasets import Dataset..
     ```

   è¿è¡Œ `main.py`ï¼ŒèŒ´é¦™è±†å°†åœ¨åˆé€‚çš„æ—¶æœºï¼Œå¯ç”¨æœç´¢å¢å¼ºã€‚

4. è°ƒå‚

   é’ˆå¯¹ä¸šåŠ¡åœºæ™¯è°ƒå‚å¾€å¾€ä¸å¯é¿å…ã€‚

   - å‚ç…§ [data.json](./tests/data.json) å¢åŠ çœŸå®æ•°æ®ï¼Œè¿è¡Œ [test_intention_prompt.py](./tests/test_intention_prompt.py) å¾—åˆ°åˆé€‚çš„ prompt å’Œé˜ˆå€¼ï¼Œæ›´æ–°è¿› [worker](./huixiangdou/service/worker.py)
   - æ ¹æ®æ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦ï¼Œè°ƒæ•´[æœç´¢ç»“æœä¸ªæ•°](./huixiangdou/service/worker.py)
   - æŒ‰ç…§åœºæ™¯åå¥½ï¼Œä¿®æ”¹ config.ini ä¸­çš„ `web_search.domain_partial_order`ï¼Œå³æœç´¢ç»“æœååº

# ğŸ› ï¸ FAQ

1. å¦‚ä½•æ¥å…¥å…¶ä»– IM ï¼Ÿ

   - ä¼ä¸šå¾®ä¿¡ã€‚è¯·æŸ¥çœ‹[ä¼ä¸šå¾®ä¿¡åº”ç”¨å¼€å‘æŒ‡å—](https://developer.work.weixin.qq.com/document/path/90594)
   - ä¸ªäººå¾®ä¿¡ã€‚æˆ‘ä»¬å·²å‘å¾®ä¿¡å›¢é˜Ÿç¡®è®¤æš‚æ—  APIï¼Œ[itchat](https://github.com/littlecodersh/ItChat) æˆ–è®¸æœ‰å¸®åŠ©ï¼ˆ**æ³¨æ„é£é™©**ï¼‰
   - é’‰é’‰ã€‚å‚è€ƒ[é’‰é’‰å¼€æ”¾å¹³å°-è‡ªå®šä¹‰æœºå™¨äººæ¥å…¥](https://open.dingtalk.com/document/robots/custom-robot-access)

2. æœºå™¨äººå¤ªé«˜å†·/å¤ªå˜´ç¢æ€ä¹ˆåŠï¼Ÿ

   - æŠŠçœŸå®åœºæ™¯ä¸­ï¼Œåº”è¯¥å›ç­”çš„é—®é¢˜å¡«å…¥`resource/good_questions.json`ï¼Œåº”è¯¥æ‹’ç»çš„å¡«å…¥`resource/bad_questions.json`
   - è°ƒæ•´ `repodir` ä¸­çš„æ–‡æ¡£ï¼Œç¡®ä¿ä¸åŒ…å«åœºæ™¯æ— å…³å†…å®¹

   é‡æ–°æ‰§è¡Œ `feature_store` æ¥æ›´æ–°é˜ˆå€¼å’Œç‰¹å¾åº“

3. å¯åŠ¨æ­£å¸¸ï¼Œä½†è¿è¡ŒæœŸé—´æ˜¾å­˜ OOM æ€ä¹ˆåŠï¼Ÿ

   åŸºäº transformers ç»“æ„çš„ LLM é•¿æ–‡æœ¬éœ€è¦æ›´å¤šæ˜¾å­˜ï¼Œæ­¤æ—¶éœ€è¦å¯¹æ¨¡å‹åš kv cache é‡åŒ–ï¼Œå¦‚ [lmdeploy é‡åŒ–è¯´æ˜](https://github.com/InternLM/lmdeploy/blob/main/docs/en/kv_int8.md)ã€‚ç„¶åä½¿ç”¨ docker ç‹¬ç«‹éƒ¨ç½² Hybrid LLM Serviceã€‚

4. å¦‚ä½•æ¥å…¥å…¶ä»– local LLM/ æ¥å…¥åæ•ˆæœä¸ç†æƒ³æ€ä¹ˆåŠï¼Ÿ

   - æ‰“å¼€ [hybrid llm service](./huixiangdou/service/llm_server_hybrid.py)ï¼Œå¢åŠ æ–°çš„ LLM æ¨ç†å®ç°
   - å‚ç…§ [test_intention_prompt å’Œæµ‹è¯•æ•°æ®](./tests/test_intention_prompt.py)ï¼Œé’ˆå¯¹æ–°æ¨¡å‹è°ƒæ•´ prompt å’Œé˜ˆå€¼ï¼Œæ›´æ–°åˆ° [worker.py](./huixiangdou/service/worker.py)

5. å“åº”å¤ªæ…¢/ç½‘ç»œè¯·æ±‚æ€»æ˜¯å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

   - å‚è€ƒ [hybrid llm service](./huixiangdou/service/llm_server_hybrid.py) å¢åŠ æŒ‡æ•°é€€é¿é‡ä¼ 
   - local LLM æ›¿æ¢ä¸º [lmdeploy](https://github.com/internlm/lmdeploy) ç­‰æ¨ç†æ¡†æ¶ï¼Œè€ŒéåŸç”Ÿçš„ huggingface/transformers

6. æœºå™¨é…ç½®ä½ï¼ŒGPU æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

   æ­¤æ—¶æ— æ³•è¿è¡Œ local LLMï¼Œåªèƒ½ç”¨ remote LLM é…åˆ text2vec æ‰§è¡Œ pipelineã€‚è¯·ç¡®ä¿ `config.ini` åªä½¿ç”¨ remote LLMï¼Œå…³é—­ local LLM

# ğŸ“ å¼•ç”¨

```shell
@misc{2023HuixiangDou,
    title={HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical Assistance},
    author={HuixiangDou Contributors},
    howpublished = {\url{https://github.com/internlm/huixiangdou}},
    year={2023}
}
```
