[feature_store]
# `feature_store.py` use this throttle to distinct `good_questions` and `bad_questions`
reject_throttle = -1.0
# text2vec model path, support local relative path and huggingface model format.
# also support local path, model_path = "/path/to/your/text2vec-model"
embedding_model_path = "BAAI/bge-m3"
reranker_model_path = "BAAI/bge-reranker-v2-minicpm-layerwise"
work_dir = "workdir"

[web_search]
engine = "serper"
# web search engine support `ddgs` and `serper`
# For ddgs, see https://pypi.org/project/duckduckgo-search
# For serper, check https://serper.dev/api-key to get a free API key
serper_x_api_key = "YOUR-API-KEY-HERE"
domain_partial_order = ["openai.com", "pytorch.org", "readthedocs.io", "nvidia.com", "stackoverflow.com", "juejin.cn", "zhuanlan.zhihu.com", "www.cnblogs.com"]
save_dir = "logs/web_search_result"

[llm]

[llm.server]
# remote LLM service configuration
# support "gpt", "kimi", "deepseek", "zhipuai", "step", "xi-api" and "alles-apin"
# xi-api and alles-apin is chinese gpt proxy

remote_type = "kimi"
remote_api_key = "YOUR-API-KEY-HERE"
# max text length for remote LLM.
# use 128000 for kimi, 192000 for gpt/xi-api, 16000 for deepseek, 128000 for zhipuai
remote_llm_max_text_length = 128000
# openai API model type, support model list:
# "auto" for kimi. To save money, we auto select model name by prompt length.
# "auto" for step to save money, see https://platform.stepfun.com
# "gpt-4-0613" for gpt/xi-api,
# "deepseek-chat" for deepseek,
# "glm-4" for zhipuai,
# "gpt-4-1106-preview" for alles-apin or OpenAOE
remote_llm_model = "auto"
# request per minute
rpm = 500

[worker]
# enable web search or not
enable_web_search = 1
# enable search enhancement or not
enable_sg_search = 0
save_path = "logs/work.txt"

[worker.time]
enable = 0
start = "00:00:00"
end = "23:59:59"
has_weekday = 1

[frontend]
# chat group assistant type, support "lark", "lark_group", "wechat_personal" and "none"
# for "lark", open https://open.feishu.cn/document/client-docs/bot-v3/add-custom-bot to add bot, **only send, cannot receive**
# for "lark_group", open https://open.feishu.cn/document/home/introduction-to-custom-app-development/self-built-application-development-process to create one
# for "wechat_personal", read ./docs/add_wechat_group_zh.md to setup gateway
type = "none"

# for "lark", it is chat group webhook url, send reply to group, for example "https://open.feishu.cn/open-apis/bot/v2/hook/xxxxxxxxxxxxxxx"
# for "lark_group", it is the url to fetch chat group message, for example "http://101.133.161.20:6666/fetch", `101.133.161.20` is your own public IPv4 addr
# for "wechat_personal", it is useless
webhook_url = "https://open.feishu.cn/open-apis/bot/v2/hook/xxxxxxxxxxxxxxx"

# when a new group chat message is received, should it be processed immediately or wait for 18 seconds in case the user hasn't finished speaking?
# support "immediate"
message_process_policy = "immediate"

[frontend.lark_group]
# "lark_group" configuration examples, use your own app_id and secret !!!
app_id = "cli_a53a34dcb778500e"
app_secret = "2ajhg1ixSvlNm1bJkH4tJhPfTCsGGHT1"
encrypt_key = "abc"
verification_token = "def"

[frontend.wechat_personal]
# "wechat_personal" listen port
bind_port = 9527
