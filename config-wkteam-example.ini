[feature_store]
# `feature_store.py` use this throttle to distinct `good_questions` and `bad_questions`
reject_throttle = -1.0
# text2vec model path, support local relative path and huggingface model format.
# also support local path, model_path = "/path/to/your/text2vec-model"
embedding_model_path = "maidalun1020/bce-embedding-base_v1"
reranker_model_path = "maidalun1020/bce-reranker-base_v1"
work_dir = "workdir"

[web_search]
engine = "serper"
# web search engine support ddgs and serper
# For ddgs, see https://pypi.org/project/duckduckgo-search
# For serper, check https://serper.dev/api-key to get a free API key
serper_x_api_key = "YOUR-API-KEY-HERE"
domain_partial_order = ["openai.com", "pytorch.org", "readthedocs.io", "nvidia.com", "stackoverflow.com", "juejin.cn", "zhuanlan.zhihu.com", "www.cnblogs.com"]
save_dir = "logs/web_search_result"

[llm]
[llm.server]
# remote LLM service configuration
# support "gpt", "kimi", "deepseek", "zhipuai", "xi-api" and "alles-apin"
# xi-api and alles-apin is chinese gpt proxy

remote_type = "kimi"
remote_api_key = "YOUR-API-KEY-HERE"
# max text length for remote LLM.
# use 128000 for kimi, 192000 for gpt/xi-api, 16000 for deepseek, 128000 for zhipuai
remote_llm_max_text_length = 128000
# openai API model type, support model list:
# use "auto" or "kimi-k2-0711-preview" for kimi. To save money, we auto select model name by prompt length.
# "gpt-4-0613" for gpt/xi-api,
# "deepseek-chat" for deepseek,
# "glm-4" for zhipuai,
# "gpt-4-1106-preview" for alles-apin or OpenAOE
remote_llm_model = "auto"
# request per minute
rpm = 500
# token per minute
tpm = 200000

[worker]
# enable web search or not
enable_web_search = 1
save_path = "logs/work.txt"

[worker.time]
enable = 0
start = "00:00:00"
end = "23:59:59"
has_weekday = 1

[frontend]
# chat group assistant type, support "lark_group", "wechat_personal", "wechat_wkteam" and "none"
# for "lark_group", open https://open.feishu.cn/document/home/introduction-to-custom-app-development/self-built-application-development-process to create one
# for "wechat_personal", read ./docs/add_wechat_group_zh.md to setup gateway
# for "wkteam", see https://wkteam.cn/
type = "none"

# for "lark", it is chat group webhook url, send reply to group, for example "https://open.feishu.cn/open-apis/bot/v2/hook/xxxxxxxxxxxxxxx"
# for "lark_group", it is the url to fetch chat group message, for example "http://101.133.161.20:6666/fetch", `101.133.161.20` is your own public IPv4 addr
# for "wechat_personal", it is useless
webhook_url = "https://open.feishu.cn/open-apis/bot/v2/hook/xxxxxxxxxxxxxxx"

# when a new group chat message is received, should it be processed immediately or wait for 18 seconds in case the user hasn't finished speaking?
# support "immediate"
message_process_policy = "immediate"

[frontend.lark_group]
# "lark_group" configuration examples, use your own app_id and secret !!!
app_id = "cli_a53a34dcb778500e"
app_secret = "2ajhg1ixSvlNm1bJkH4tJhPfTCsGGHT1"
encrypt_key = "abc"
verification_token = "def"

[frontend.wechat_personal]
# "wechat_personal" listen port
bind_port = 9527

[frontend.wechat_wkteam]
account = "YOUR-MOBILE-NUMBER"
password = "YOUR-PASSWD-HERE"
proxy = 3
dir = "wkteam"
callback_ip = "101.133.161.20"
callback_port = 9528
redis_host = "101.133.161.20"
redis_port = "6380"
redis_passwd = "hxd123"

# 群号和介绍
# 茴香豆相关
[frontend.wechat_wkteam.44546611710]
name = "茴香豆群（立夏）"
introduction = "github https://github.com/InternLM/HuixiangDou 用户体验群"

[frontend.wechat_wkteam.38720590618]
name = "茴香豆群（惊蛰）"
introduction = "github https://github.com/InternLM/HuixiangDou 用户体验群"

[frontend.wechat_wkteam.48437885473]
name = "茴香豆群（谷雨）"
introduction = "github https://github.com/InternLM/HuixiangDou 用户体验群"

[frontend.wechat_wkteam.34744063953]
name = "茴香豆群（雨水）"
introduction = "github https://github.com/InternLM/HuixiangDou 用户体验群"

# ncnn contributors
[frontend.wechat_wkteam.18356748488]
name = "卷卷群"
introduction = "ncnn contributors group"
