# HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical Assistance

<small> ç®€ä½“ä¸­æ–‡ | [English](README_en.md) </small>

[![GitHub license](https://img.shields.io/badge/license-BSD--3--Clause-brightgreen.svg)](./LICENSE)
![CI](https://img.shields.io/github/actions/workflow/status/internml/huixiangdou/lint.yml?branch=master)

â€œèŒ´é¦™è±†â€æ˜¯ä¸€ä¸ªåŸºäº LLM çš„é¢†åŸŸç‰¹å®šçŸ¥è¯†åŠ©æ‰‹ã€‚ç‰¹ç‚¹ï¼š
1. åº”å¯¹ç¾¤èŠè¿™ç±»å¤æ‚åœºæ™¯ï¼Œè§£ç­”ç”¨æˆ·é—®é¢˜çš„åŒæ—¶ï¼Œä¸ä¼šæ¶ˆæ¯æ³›æ»¥
2. æå‡ºä¸€å¥—è§£ç­”æŠ€æœ¯é—®é¢˜çš„ç®—æ³• pipeline
3. éƒ¨ç½²æˆæœ¬ä½ï¼Œåªéœ€è¦ LLM æ¨¡å‹æ»¡è¶³ 4 ä¸ª trait å³å¯è§£ç­”å¤§éƒ¨åˆ†ç”¨æˆ·é—®é¢˜ï¼Œè§[æŠ€æœ¯æŠ¥å‘Š](./resource/HuixiangDou.pdf)

æŸ¥çœ‹[èŒ´é¦™è±†å·²è¿è¡Œåœ¨å“ªäº›åœºæ™¯](./huixiangdou-inside.md)ã€‚

# ğŸ”¥ è¿è¡Œ

æˆ‘ä»¬å°†ä»¥ lmdeploy & mmpose ä¸ºä¾‹ï¼Œä»‹ç»å¦‚ä½•æŠŠçŸ¥è¯†åŠ©æ‰‹éƒ¨ç½²åˆ°é£ä¹¦ç¾¤

## STEP1. å»ºç«‹è¯é¢˜ç‰¹å¾åº“
```bash
# ä¸‹è½½èŠå¤©è¯é¢˜
mkdir repodir
git clone https://github.com/openmmlab/mmpose --depth=1 repodir/mmpose
git clone https://github.com/internlm/lmdeploy --depth=1 repodir/lmdeploy

# å»ºç«‹ç‰¹å¾åº“
cd HuixiangDou && mkdir workdir # åˆ›å»ºå·¥ä½œç›®å½•
python3 -m pip install -r requirements.txt # å®‰è£…ä¾èµ–
python3 service/feature_store.py repodir workdir # æŠŠ repodir çš„ç‰¹å¾ä¿å­˜åˆ° workdir
```
è¿è¡Œç»“æŸåï¼ŒèŒ´é¦™è±†èƒ½å¤ŸåŒºåˆ†åº”è¯¥å¤„ç†å“ªäº›ç”¨æˆ·è¯é¢˜ï¼Œå“ªäº›é—²èŠåº”è¯¥æ‹’ç»ã€‚è¯·ç¼–è¾‘ [good_questions](./resource/good_questions.json) å’Œ [bad_questions](./resource/bad_questions.json)ï¼Œå°è¯•è‡ªå·±çš„é¢†åŸŸçŸ¥è¯†ï¼ˆåŒ»ç–—ï¼Œé‡‘èï¼Œç”µåŠ›ç­‰ï¼‰ã€‚

```bash
# æ¥å—æŠ€æœ¯è¯é¢˜
process query: mmdeploy ç°åœ¨æ”¯æŒ mmtrack æ¨¡å‹è½¬æ¢äº†ä¹ˆ
process query: æœ‰å•¥ä¸­æ–‡çš„ text to speech æ¨¡å‹å—?
# æ‹’ç»é—²èŠ
reject query: ä»Šå¤©ä¸­åˆåƒä»€ä¹ˆï¼Ÿ
reject query: èŒ´é¦™è±†æ˜¯æ€ä¹ˆåšçš„
```

## STEP2. è¿è¡ŒåŸºç¡€ç‰ˆæŠ€æœ¯åŠ©æ‰‹

**é…ç½®å…è´¹ TOKEN**

èŒ´é¦™è±†ä½¿ç”¨äº†æœç´¢å¼•æ“ï¼Œç‚¹å‡» [serper å®˜ç½‘](https://serper.dev/api-key)è·å–é™é¢ WEB_SEARCH_TOKENï¼Œå¡«å…¥ `config.ini`

```shell
# config.ini
..
[web_search]
x_api_key = "${YOUR-X-API-KEY}"
..
```

**æµ‹è¯•é—®ç­”æ•ˆæœ**

è¯·ä¿è¯ GPU æ˜¾å­˜è¶…è¿‡ 20GBï¼ˆå¦‚ 3090 åŠä»¥ä¸Šï¼‰ï¼Œè‹¥æ˜¾å­˜è¾ƒä½è¯·æŒ‰ FAQ ä¿®æ”¹ã€‚

é¦–æ¬¡è¿è¡Œå°†è‡ªåŠ¨ä¸‹è½½é…ç½®ä¸­çš„ internlm2-7B å’Œ text2vec-large-chineseï¼Œè¯·ä¿è¯ç½‘ç»œç•…é€šã€‚

  * **é docker ç”¨æˆ·**ã€‚å¦‚æœä½ **ä¸**ä½¿ç”¨ docker ç¯å¢ƒï¼Œå¯ä»¥ä¸€æ¬¡å¯åŠ¨æ‰€æœ‰æœåŠ¡ã€‚
    ```bash
    # standalone
    python3 main.py workdir --standalone
    ..
    ErrorCode.SUCCESS, è¦å®‰è£… MMDeployï¼Œé¦–å…ˆéœ€è¦å‡†å¤‡ä¸€ä¸ªæ”¯æŒ Python 3.6+ å’Œ PyTorch 1.8+ çš„ç¯å¢ƒã€‚ç„¶åï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®‰è£… MMDeploy..
    ```

  * **docker ç”¨æˆ·**ã€‚å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ dockerï¼Œ`HuixiangDou` çš„ Hybrid LLM Service åˆ†ç¦»éƒ¨ç½²ã€‚
    ```bash
    # å¯åŠ¨æœåŠ¡
    python3 service/llm_server_hybride.py
    ```
    æ‰“å¼€æ–°ç»ˆç«¯ï¼ŒæŠŠ host IP é…ç½®è¿› `config.ini`ï¼Œè¿è¡Œ

    ```bash
    # config.ini
    [llm]
    ..
    client_url = "http://10.140.24.142:39999/inference" # ä¸¾ä¾‹

    python3 main.py workdir
    ```

## STEP3.é›†æˆåˆ°é£ä¹¦[å¯é€‰]

ç‚¹å‡»[åˆ›å»ºé£ä¹¦è‡ªå®šä¹‰æœºå™¨äºº](https://open.feishu.cn/document/client-docs/bot-v3/add-custom-bot)ï¼Œè·å–å›è°ƒ WEBHOOK_URLï¼Œå¡«å†™åˆ° config.ini

```shell
# config.ini
..
[frontend]
type = "lark"
webhook_url = "${YOUR-LARK-WEBHOOK-URL}"
```

è¿è¡Œã€‚ç»“æŸåï¼ŒæŠ€æœ¯åŠ©æ‰‹çš„ç­”å¤å°†å‘é€åˆ°é£ä¹¦ç¾¤ã€‚
```shell
python3 main.py workdir
```
<img src="./resource/figures/lark-example.png" width="400">

å¦‚æœè¿˜éœ€è¦è¯»å–é£ä¹¦ç¾¤æ¶ˆæ¯ï¼Œè§[é£ä¹¦å¼€å‘è€…å¹¿åœº-æ·»åŠ åº”ç”¨èƒ½åŠ›-æœºå™¨äºº](https://open.feishu.cn/app?lang=zh-CN)ã€‚

## STEP4.é«˜ç²¾åº¦é…ç½®[å¯é€‰]
ä¸ºäº†è¿›ä¸€æ­¥æå‡åŠ©æ‰‹çš„ç­”å¤ä½“éªŒï¼Œä»¥ä¸‹ç‰¹æ€§ï¼Œå¼€å¯å¾—è¶Šå¤šè¶Šå¥½ã€‚

1. ä½¿ç”¨æ›´é«˜ç²¾åº¦ local LLM

    æŠŠ config.ini ä¸­çš„`llm.local` æ¨¡å‹è°ƒæ•´ä¸º `internlm2-20B`
    æ­¤é€‰é¡¹æ•ˆæœæ˜¾è‘—ï¼Œä½†éœ€è¦æ›´å¤§çš„ GPU æ˜¾å­˜ã€‚

2. Hybrid LLM Service

    å¯¹äºæ”¯æŒ openai æ¥å£çš„ LLM æœåŠ¡ï¼ŒèŒ´é¦™è±†å¯ä»¥å‘æŒ¥å®ƒçš„ Long Context èƒ½åŠ›ã€‚
    ä»¥ kimi ä¸ºä¾‹ï¼Œä»¥ä¸‹æ˜¯ `config.ini` é…ç½®ç¤ºä¾‹ï¼š

    ```bash
    # config.ini
    [llm.server]
    ..
    # open https://platform.moonshot.cn/
    remote_type = "kimi"
    remote_api_key = "YOUR-KIMI-API-KEY"
    remote_llm_max_text_length = 128000
    remote_llm_model = "moonshot-v1-128k"
    ```
    æˆ‘ä»¬åŒæ ·æ”¯æŒ gpt APIã€‚æ³¨æ„æ­¤ç‰¹æ€§ä¼šå¢åŠ å“åº”è€—æ—¶å’Œè¿è¡Œæˆæœ¬ã€‚

3. repo æœç´¢å¢å¼º

    æ­¤ç‰¹æ€§é€‚åˆå¤„ç†ç–‘éš¾é—®é¢˜ï¼Œéœ€è¦åŸºç¡€å¼€å‘èƒ½åŠ›è°ƒæ•´ promptã€‚

    * ç‚¹å‡» [sourcegraph-settings-access](https://sourcegraph.com/users/tpoisonooo/settings/tokens) è·å– token

        ```bash
        # open https://github.com/sourcegraph/src-cli#installation
        curl -L https://sourcegraph.com/.api/src-cli/src_linux_amd64 -o /usr/local/bin/src && chmod +x /usr/local/bin/src

        # æŠŠ token å¡«å…¥ config.ini
        [sg_search]
        ..
        src_access_token = "${YOUR_ACCESS_TOKEN}"
        ```
    
    * ç¼–è¾‘ repo çš„åå­—å’Œç®€ä»‹ï¼Œæˆ‘ä»¬ä»¥ opencompass ä¸ºä¾‹

        ```bash
        # config.ini
        # add your repo here, we just take opencompass and lmdeploy as example
        [sg_search.opencompass]
        github_repo_id = "open-compass/opencompass"
        introduction = "ç”¨äºè¯„æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰.."
        ```
    
    * ä½¿ç”¨ `python3 service/sg_search.py` å•æµ‹ï¼Œè¿”å›å†…å®¹åº”åŒ…å« opencompass æºç å’Œæ–‡æ¡£
  
       ```bash
       python3 service/sg_search.py
       ..
       "filepath": "opencompass/datasets/longbench/longbench_trivia_qa.py",
       "content": "from datasets import Dataset..
       ```

    è¿è¡Œ `main.py`ï¼ŒèŒ´é¦™è±†å°†åœ¨åˆé€‚çš„æ—¶æœºï¼Œå¯ç”¨æœç´¢å¢å¼ºã€‚


# ğŸ› ï¸ FAQ 

1. å¦‚ä½•æ¥å…¥å…¶ä»– IM ï¼Ÿ
    * å¾®ä¿¡ã€‚ä¼ä¸šå¾®ä¿¡è¯·æŸ¥çœ‹[ä¼ä¸šå¾®ä¿¡åº”ç”¨å¼€å‘æŒ‡å—](https://developer.work.weixin.qq.com/document/path/90594)ï¼›å¯¹äºä¸ªäººå¾®ä¿¡ï¼Œæˆ‘ä»¬å·²å‘å¾®ä¿¡å›¢é˜Ÿç¡®è®¤æš‚æ—  APIï¼Œé¡»è‡ªè¡Œæœç´¢å­¦ä¹ 
    * é’‰é’‰ã€‚å‚è€ƒ[é’‰é’‰å¼€æ”¾å¹³å°-è‡ªå®šä¹‰æœºå™¨äººæ¥å…¥](https://open.dingtalk.com/document/robots/custom-robot-access)

2. æœºå™¨äººå¤ªé«˜å†·/å¤ªå˜´ç¢æ€ä¹ˆåŠï¼Ÿ

    * æŠŠçœŸå®åœºæ™¯ä¸­ï¼ŒæŠŠåº”è¯¥å›ç­”çš„é—®é¢˜å¡«å…¥`resource/good_questions.json`ï¼Œåº”è¯¥æ‹’ç»çš„å¡«å…¥`resource/bad_questions.json`
    * è°ƒæ•´ `repodir` ä¸­çš„ä¸»é¢˜å†…å®¹ï¼Œç¡®ä¿åº•åº“çš„ markdown æ–‡æ¡£ä¸åŒ…å«åœºæ™¯æ— å…³å†…å®¹

    é‡æ–°æ‰§è¡Œ `service/feature_store.py`ï¼Œæ›´æ–°é˜ˆå€¼å’Œç‰¹å¾åº“

3. å¯åŠ¨æ­£å¸¸ï¼Œä½†è¿è¡ŒæœŸé—´æ˜¾å­˜ OOM æ€ä¹ˆåŠï¼Ÿ

    åŸºäº transformers ç»“æ„çš„ LLM é•¿æ–‡æœ¬éœ€è¦æ›´å¤šæ˜¾å­˜ï¼Œæ­¤æ—¶éœ€è¦å¯¹æ¨¡å‹åš kv cache é‡åŒ–ï¼Œå¦‚ [lmdeploy é‡åŒ–è¯´æ˜](https://github.com/InternLM/lmdeploy/blob/main/docs/en/kv_int8.md)ã€‚ç„¶åä½¿ç”¨ docker ç‹¬ç«‹éƒ¨ç½² Hybrid LLM Serviceã€‚

4. å¦‚ä½•æ¥å…¥å…¶ä»– local LLM/ æ¥å…¥åæ•ˆæœä¸ç†æƒ³æ€ä¹ˆåŠï¼Ÿ

    * æ‰“å¼€ [hybrid llm service](./service/llm_server_hybrid.py)ï¼Œå¢åŠ æ–°çš„ LLM æ¨ç†å®ç°
    * å‚ç…§ [test_intention_prompt å’Œæµ‹è¯•æ•°æ®](./tests/test_intention_prompt.py)ï¼Œé’ˆå¯¹æ–°æ¨¡å‹è°ƒæ•´ prompt å’Œé˜ˆå€¼ï¼Œæ›´æ–°åˆ° [worker.py](./service/worker.py)

5. å“åº”å¤ªæ…¢/è¯·æ±‚æ€»æ˜¯å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

    * å‚è€ƒ [hybrid llm service](./service/llm_server_hybrid.py) å¢åŠ æŒ‡æ•°é€€é¿é‡ä¼ 
    * local LLM æ›¿æ¢ä¸º [lmdeploy](https://github.com/internlm/lmdeploy) ç­‰æ¨ç†æ¡†æ¶ï¼Œè€ŒéåŸç”Ÿçš„ huggingface/transformers
      
5. GPU æ˜¾å­˜å¤ªä½æ€ä¹ˆåŠï¼Ÿ

    æ­¤æ—¶æ— æ³•è¿è¡Œ local LLMï¼Œåªèƒ½ç”¨ remote LLM é…åˆ text2vec æ‰§è¡Œ pipelineã€‚è¯·ç¡®ä¿ `config.ini` åªä½¿ç”¨ remote LLMï¼Œå…³é—­ local LLM

# ğŸ“ License
é¡¹ç›®ä½¿ç”¨ [BSD 3-Clause License](./LICENSE)
