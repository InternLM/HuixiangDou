from BCEmbedding import EmbeddingModel
from sentence_transformers import SentenceTransformer

# (Pdb) torch.__version__
# '2.2.2+cu121'

# (Pdb) import sentence_transformers
# (Pdb) sentence_transformers.__version__
# '3.0.1'

# list of sentences
sentences = ['大佬们，请问如何安装mmcv?\n']
# sentences = [query.text]

# init embedding model
model1 = SentenceTransformer(model_name_or_path='/data2/khj/bce-embedding-base_v1')
emb1 = model1.encode(sentences, normalize_embeddings=True)
print(emb1[0, 0:100])
# [ 2.27064490e-02  6.52235700e-03 -6.60670735e-03  2.82378905e-02
#  -4.23520654e-02  3.50253433e-02 -3.75928059e-02  4.39181253e-02
#   2.07435223e-03  7.78974686e-03 -1.62818842e-02 -7.12286914e-03
#   7.55418167e-02 -1.32979536e-02 -4.25908640e-02  5.17920293e-02
#  -7.29086921e-02  1.92712229e-02  3.08401737e-04  2.78450572e-03
#   8.07866603e-02 -2.95906160e-02  1.20137021e-01  2.15978641e-02
#   5.42808920e-02 -1.40500171e-02 -2.30203178e-02  2.66241953e-02
#  -4.54075523e-02  7.94132706e-03 -7.61417393e-03 -2.65015271e-02
#   3.59652378e-02  4.75489870e-02 -2.02227868e-02 -1.92782935e-02
#  -1.51286395e-02 -5.89573607e-02 -9.06837434e-02  4.33626957e-02
#   3.53859067e-02  2.16610935e-02 -3.04897577e-02  3.32642421e-02
#  -1.26434257e-02 -3.17539996e-03  5.38454531e-03  2.33619660e-02
#   2.76829731e-02  1.81363951e-02  5.62918335e-02 -4.35542352e-02
#   1.82253513e-02  3.56561653e-02  6.30606860e-02 -2.95952503e-02
#  -7.61354575e-03 -1.15259029e-02  1.00938044e-02 -1.03235366e-02
#  -1.67877134e-02 -1.12033095e-02 -4.99769486e-02 -1.44557441e-02
#  -2.29974966e-02 -2.01950204e-02 -2.06282530e-02 -1.91477686e-02
#   1.20543549e-02  2.68511102e-02  4.25181873e-02  3.57608870e-02
#  -1.33546710e-03  4.03872021e-02 -2.76293196e-02  3.64267617e-03
#  -4.17132629e-03 -1.73272435e-02 -2.44494136e-02 -1.61279812e-02
#   7.76542164e-03  9.57545731e-03  2.58920640e-02 -2.26965714e-02
#  -7.54475041e-05 -1.75510086e-02 -1.38453580e-02  5.27697206e-02
#  -1.63486097e-02  1.69499014e-02 -1.11275297e-02 -1.07301818e-03
#  -7.06254854e-04 -9.24953539e-03 -9.37404204e-03 -2.96557285e-02
#  -1.49586275e-02 -5.31528378e-03 -3.70575525e-02 -1.99552961e-02]


# fp32
model2 = EmbeddingModel(model_name_or_path='/data2/khj/bce-embedding-base_v1')
emb2 = model2.encode(sentences, normalize_to_unit=True)
print(emb2[0, 0:100])                                                                                                                              | 0/1 [00:00<?, ?it/s]
# [ 0.02495519  0.00145093 -0.00357047  0.02271552 -0.04381615  0.04257036
#  -0.04001398  0.04999371  0.00657894  0.01279035  0.00517301 -0.01259502
#   0.0859137  -0.02055852 -0.04963807  0.06536899 -0.05691828  0.01796977
#  -0.00239803  0.00212741  0.06606179 -0.03382375  0.10876276  0.00875535
#   0.05429256 -0.01759921 -0.01917351  0.02298737 -0.03785134  0.01124877
#  -0.01599116 -0.02915872  0.0371046   0.06280778 -0.03448223 -0.00651793
#  -0.01752605 -0.05859256 -0.07491301  0.03073437  0.03724524  0.03757126
#  -0.04640587  0.03524466 -0.01691931 -0.0099074   0.03141229  0.02837254
#   0.02189152  0.02024683  0.05513586 -0.04465799  0.02600198  0.04792593
#   0.08144734 -0.01666879 -0.00103528 -0.01757018  0.02530343 -0.00843846
#  -0.01203103 -0.0161062  -0.04621442 -0.00938876 -0.02546393 -0.01491435
#  -0.01928305 -0.02440765  0.00451228  0.02867517  0.04371059  0.03615013
#   0.00200925  0.02356826 -0.04723409  0.00812949  0.00021193 -0.03553445
#  -0.01616245 -0.00649725 -0.00134869  0.00298218  0.03786474 -0.01578123
#  -0.00277072 -0.03315896 -0.00208498  0.06014286 -0.02470733  0.02014162
#  -0.01798468  0.00464859 -0.01090737 -0.0263143  -0.0091747  -0.01841594
#  -0.01608938 -0.00095074 -0.03385591 -0.02494052]

